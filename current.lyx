#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass amsart
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Nonparametric prior estimation from cohort data and its application to Systems
 Biology
\end_layout

\begin_layout Abstract
This thesis will cover the theory and application of Empirical Bayes Methods
 for parameter estimation in a Bayesian setting for cohortdata in combination
 with MCMC Sampling.
\end_layout

\begin_layout Abstract
In the *basics* section we will introduce the Bayesian framework
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Since the invention of the computer Bayesian methods, beforehand often untractab
le to compute, have gained a great amount of importance in the field of
 inverse problems.
\end_layout

\begin_layout Standard
Now, with the Internet of things coming and the progressing digitalisation
 of scienes, healthcare and *, ever greater amounts of data is beeing gathered,
 coining the term big data.
\end_layout

\begin_layout Standard
This poses new challenges for storage and performance, but also introduces
 problems of errorneous, incomplete and diverse/cohort * data raising the
 demand for adjusted analytical methods.
\end_layout

\begin_layout Standard
By its nature the bayesian formalism is very well suited to handle uncertainty
 and missing information and the here presented method fits (cohort parallelisat
ion..*)
\end_layout

\begin_layout Standard
Taking a nonparametric, sampling based approch allows the application of
 this method to a wide class of problems.
\end_layout

\begin_layout Standard
The Bayesians strength but also weakness is the need to incororate prior
 belief/knowledge about the system in question.
 Whilst the choice of the prior remains a question of debate, we will use
 cohort data, e.g.
 measurements of multiple persons, to improve our prior knowledge by incorporati
ng the individual posteriors into a new, informed*, prior.
\end_layout

\begin_layout Section
Theory
\end_layout

\begin_layout Subsection
The Bayesian formalism
\end_layout

\begin_layout Standard
We start by laying out the basic formal tools in the Bayesian setting.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 denote continuous random variables and 
\begin_inset Formula $\rho\left(x,y\right)$
\end_inset

 their joint probability density .
\end_layout

\begin_layout Definition
The 
\emph on
conditional probability
\emph default
 density of 
\begin_inset Formula $Y$
\end_inset

 given the occurence of the value 
\begin_inset Formula $x$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

 is
\begin_inset Formula 
\[
\rho\left(y|X=x\right):=\frac{\rho\left(x,y\right)}{\rho\left(x\right)},
\]

\end_inset

where 
\begin_inset Formula $\rho\left(x\right)$
\end_inset

 is the 
\emph on
marginal density
\emph default
 of 
\begin_inset Formula $x$
\end_inset

, i.e.
 the density of 
\begin_inset Formula $x$
\end_inset

 marginalized over all possible 
\begin_inset Formula $y$
\end_inset

:
\begin_inset Formula 
\[
\rho\left(x\right):=\int_{y}\rho\left(x,y\right)\mathrm{d}y.
\]

\end_inset


\end_layout

\begin_layout Definition
Succesive insertion of these identities leads to the probability density
 form of 
\emph on
Bayes' theorem
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{equation}
\rho\left(x|Y=y\right):=\frac{\rho\left(x,y\right)}{\rho\left(y\right)}=\frac{\rho\left(y|x\right)\rho\left(x\right)}{\rho\left(y\right)}=\frac{\rho\left(y|x\right)\rho\left(x\right)}{\int_{x}\rho\left(y|x'\right)\rho\left(x'\right)\mathrm{d}x'}.\label{eq:bayes}
\end{equation}

\end_inset

This formula, constituting the heart of Bayesian statistics, tells us how
 to reconstruct the 
\emph on
posterior distribution
\emph default

\begin_inset Formula $\rho\left(x|y\right)$
\end_inset

 of the unknown parameter 
\begin_inset Formula $x$
\end_inset

 given data 
\begin_inset Formula $y$
\end_inset

, using the 
\emph on
likelihood
\emph default
 
\begin_inset Formula $\rho\left(y|x\right)$
\end_inset

 of 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

 as well as the 
\emph on
prior
\emph default
 
\begin_inset Formula $\rho\left(x\right)$
\end_inset

 reflecting our prior assumptions on the density of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
Note that for fixed 
\begin_inset Formula $y$
\end_inset

 the prior and posterior both are probability distributions in 
\begin_inset Formula $x$
\end_inset

, whilst the likelihood would be in 
\begin_inset Formula $y$
\end_inset

 but is not in 
\begin_inset Formula $x$
\end_inset

, which is why it is often called the 
\emph on
likelihood function
\emph default
 for emphasis.
\end_layout

\begin_layout Standard
Also note that the denominator, the 
\emph on
evidence,
\emph default
 does not depend on 
\begin_inset Formula $x$
\end_inset

 and thus is merely a scaling constant, preserving the probability density
 property of having total measure one.
 This will come in handy later for Markov Chain Monte Carlo sampling, since
 we will be able to omit it in crucial calculations.
\end_layout

\begin_layout Subsection
The model
\end_layout

\begin_layout Standard
Our inference bases on the combination of a deterministic physical model,
 our description of the reality, with a stochastic measurement error and
 the formalism of Bayes'.
 The phyiscal model is represented as a map 
\begin_inset Formula $\Phi:X\rightarrow Y$
\end_inset

, mapping some parameter 
\begin_inset Formula $x\in X=\mathbb{R}^{n}$
\end_inset

 to a resulting state 
\begin_inset Formula $y\in Y=\mathbb{R}^{m}$
\end_inset

.
 We furthermore model the the data generating measurement process as an
 independet Gaussian perturbation with known covariance 
\begin_inset Formula $\Sigma$
\end_inset

 of that state:
\begin_inset Formula 
\[
Z\left(z\mid X=x\right)=\Phi\left(x\right)+E,\quad E\overset{indep.}{\sim}\mathcal{N}\left(0,\Sigma\right)
\]

\end_inset

or shorthand
\begin_inset Formula 
\[
Z\left(z\mid x\right)\sim\mathcal{N}\left(\Phi\left(x\right),\Sigma\right).
\]

\end_inset


\end_layout

\begin_layout Standard
This 
\emph on
likelihood model 
\emph default
gives us the means to compute the probabilty of measuring a certain measurement
 
\begin_inset Formula $z$
\end_inset

, given the underlying parameter 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Standard
Assuming the 
\emph on
prior distribution
\emph default
 
\begin_inset Formula $X$
\end_inset

 was known, this would enable us to compute the 
\emph on
posterior 
\begin_inset Formula $X(x|z)$
\end_inset

 
\emph default
given some measurement 
\begin_inset Formula $z$
\end_inset

 by straightforward application of the Bayes' theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bayes"

\end_inset

.
\end_layout

\begin_layout Subsection
The empirical bayes model
\end_layout

\begin_layout Standard
Since in general the 
\emph on
prior 
\emph default

\begin_inset Formula $X$
\end_inset

 cannot assumed to known a number of different methods have been established
 for estimating this prior based on empirical cohort data, giving rise to
 the so called 
\emph on
Empirical Bayes methods.
 
\emph default
In that context the prior 
\begin_inset Formula $p\left(x\right)$
\end_inset

 itsself is seen as a hyperparameter with unknown true value 
\begin_inset Formula $\pi_{true}\in\mathcal{M}_{1}\left(\mathcal{X}\right)$
\end_inset

, giving rise to our measurements, which we want to estimate.
\end_layout

\begin_layout Standard
This corresponds to the hierarchical model 
\begin_inset Formula $\Pi\rightarrow X\rightarrow Z$
\end_inset

 in which we want to estimate 
\begin_inset Formula $\pi_{true}\in\Pi$
\end_inset

 given either finite data 
\begin_inset Formula 
\[
\bm{z}=\left(z_{m}\in\mathcal{Z}\right)_{m=1,...,M},\,z_{m}\sim Z|\pi_{true},
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
notation
\end_layout

\end_inset

 or inifinite data in the form of a probability distribution 
\begin_inset Formula 
\[
p_{Z}\sim Z|\pi_{true}.
\]

\end_inset


\end_layout

\begin_layout Standard
The corresponding evidence to a given prior 
\begin_inset Formula $\Pi=\pi$
\end_inset

 is then given by 
\begin_inset Note Note
status open

\begin_layout Plain Layout
evidence? marg likelihood?
\end_layout

\end_inset


\begin_inset Formula 
\[
p_{Z}\left(z\mid\Pi=\pi\right)=\int_{\mathcal{X}}p_{Z}\left(z\mid X=x\right)\pi\left(x\right)\mathrm{d}x.
\]

\end_inset


\end_layout

\begin_layout Subsection
Identifiability
\begin_inset CommandInset label
LatexCommand label
name "sub:Identifiability"

\end_inset


\end_layout

\begin_layout Standard
We call a marginal likelihood model 
\begin_inset Formula $p_{Z}\left(\cdot\mid\Pi=\pi\right)$
\end_inset

 identifiable, if
\begin_inset Formula 
\begin{equation}
p_{Z}\left(\cdot\mid\Pi=\pi\right)=p_{Z}\left(\cdot\mid\Pi=\pi_{true}\right)\Rightarrow\pi=\pi_{true}.\label{eq:ident}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
If this condition is not fulfilled, different priors can lead to the same
 measurements and therefore we cannot expect to reconstruct the correct
 one from the given data.
 The best we can hope for in this case is to recover a candidate of the
 equivalence class of priors leading to the same measurements, specified
 via the equivalence relation:
\begin_inset Formula 
\begin{equation}
\pi\sim\pi':\iff\left\Vert p_{Z}\left(\cdot\mid\Pi=\pi\right)-p_{Z}\left(\cdot\mid\Pi=\pi'\right)\right\Vert _{L^{1}\left(\mathcal{Z}\right)}=0.\label{eq:equiv}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Prior estimation
\end_layout

\begin_layout Standard
Assuming that the whole measurement distribution 
\begin_inset Formula $p_{Z}$
\end_inset

 is known, we introduce the 
\emph on

\begin_inset Quotes eld
\end_inset

infinite data
\begin_inset Quotes erd
\end_inset

 log-likelihood
\begin_inset Formula 
\[
L_{cc}\left(\pi\right):=-H^{cross}\left(p_{Z},p_{Z}\left(\cdot\mid\Pi=\pi\right)\right)=\int p_{Z}\log p_{Z}\left(z\mid\Pi=\pi\right)\mathrm{d}z
\]

\end_inset


\end_layout

\begin_layout Standard
as optimization criterion to estimate the true prior 
\begin_inset Formula $\pi_{true}$
\end_inset

.
\end_layout

\begin_layout Standard
While formally inspired as an adaption of the log-likelihood for finite
 data, 
\begin_inset Formula 
\[
L_{cd}\left(\pi\right)=\log\left(\prod_{m=1}^{M}p_{Z}\left(z_{m}\mid\Pi=\pi\right)\right)=\frac{1}{M}\sum_{m=1}^{M}\log p_{Z}\left(z_{m}\mid\Pi=\pi\right),
\]

\end_inset

to the 
\begin_inset Quotes eld
\end_inset

infinite data
\begin_inset Quotes erd
\end_inset

 regime, the following proposition shows that this criterion indeed leads
 to restoration of the desired true prior up to the equivalence considerations
 from 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Identifiability"

\end_inset

.
\end_layout

\begin_layout Proposition
Let 
\begin_inset Formula $\pi\in\mathcal{M}_{1}\left(\mathcal{X}\right)$
\end_inset

 be a globally supported probability density function.
 Then the following two statements are equivalent:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $p_{Z}\left(\cdot\mid\Pi=\pi\right)=p_{Z}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\pi$
\end_inset

 maximizes 
\begin_inset Formula $L_{cc}\left(\pi\right)$
\end_inset


\end_layout

\begin_layout Proof
The cross entropy of the two densities is minimal if and only if the two
 densities agree modulo 
\begin_inset Formula $\sim$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Ausfuehrlicher
\end_layout

\end_inset


\end_layout

\begin_layout Proof
In other words, maximization of 
\begin_inset Formula $L_{cc}$
\end_inset

 gives us the prior that reconstructs the true evidence 
\begin_inset Formula $p_{Z}$
\end_inset

, which according to the identifiability considerations is all we can hope
 for.
\end_layout

\begin_layout Subsection
Regularization
\end_layout

\begin_layout Subsection
Numerical realization
\end_layout

\begin_layout Standard
Since we cannot compute the occuring integrals exactly in general, we propose
 the following approximations to discretize the spaces 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\mathcal{Z}$
\end_inset

:
\end_layout

\begin_layout Enumerate
Seeing that in reality only finitely many measurements 
\begin_inset Formula $\bm{z}$
\end_inset

 are available, we approximate the density 
\begin_inset Formula $p_{Z}$
\end_inset

 by
\begin_inset Formula 
\[
p_{Z}\approx\frac{1}{\#\bm{z}}\sum_{z\in\bm{z}}\delta_{z}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
For the integration over the potentially high-dimensional parameter space
 
\begin_inset Formula $\mathcal{X}$
\end_inset

 we choose another Monte-Carlo approximation, corresponding to an importance
 sampling 
\begin_inset Formula $\bm{x=}\left(x_{k}\in\mathcal{X}\right)_{k=1}^{K}$
\end_inset

 with weights 
\begin_inset Formula $\bm{w}=\left(w_{k}\right)_{k=1}^{K},\quad w_{k}\ge0,\quad\sum_{k=1}^{K}w_{k}=1$
\end_inset

:
\begin_inset Formula 
\[
\pi\approx\sum_{k=1}^{K}w_{k}\delta_{x_{k}}.
\]

\end_inset


\end_layout

\begin_layout Section
Application
\end_layout

\begin_layout Subsection
The Model
\end_layout

\begin_layout Subsection
Sampling
\end_layout

\begin_layout Subsection
Prior estimation
\end_layout

\begin_layout Subsection
Results
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Section*
Appendix
\end_layout

\begin_layout Subsection*
Implementation
\end_layout

\end_body
\end_document
