#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrartcl
\begin_preamble
\DeclareMathOperator{\Ima}{Im}
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\usepackage[backend=bibtex,maxbibnames=4]{biblatex}
\addbibresource{refs.bib}
\fancyhf{}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing other 1.3
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 4cm
\rightmargin 3cm
\bottommargin 4cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\shape up
Master's Thesis
\begin_inset Newline newline
\end_inset


\begin_inset VSpace 1cm*
\end_inset


\shape default
\size larger
An Information-Theoretic Empirical Bayes Method 
\begin_inset Newline newline
\end_inset

and its Application to a Systems Biology Model
\shape up
\size default

\begin_inset VSpace 1.5cm*
\end_inset


\end_layout

\begin_layout Subtitle
\begin_inset Graphics
	filename siegel.png
	display false
	width 7cm

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset VSpace 0.5cm*
\end_inset

Free University of Berlin
\begin_inset Newline newline
\end_inset

Department of Mathematics and Computer Science
\begin_inset VSpace 1cm*
\end_inset


\end_layout

\begin_layout Author
Alexander Sikorski
\begin_inset Newline newline
\end_inset

Supervisor: Prof.
 Susanna RÃ¶blitz 
\end_layout

\begin_layout Date
Berlin 2017
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Declaration
\end_layout

\begin_layout Standard
I hereby declare that this thesis is my own work and has not been submitted
 in any form for another degree or diploma at any university or other institute.
 Information derived from the published and unpublished work of others has
 been acknowledged in the text and a list of references is given.
 
\end_layout

\begin_layout Standard
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Standard
Alexander Sikorski
\end_layout

\begin_layout Standard
Berlin, March 14th, 2017
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{page}{1}
\backslash
fancyhf[FRO,FLE]{
\backslash
thepage}
\end_layout

\begin_layout Plain Layout


\backslash
fancyhead[LO]{
\backslash
leftmark}
\end_layout

\begin_layout Plain Layout


\backslash
fancyhead[RE]{
\backslash
rightmark} 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
This thesis will cover the development and application of an empirical Bayes
 method to the problem of parameter estimation in systems biology.
 The goal is to provide a general and practical solution to the Bayesian
 inverse problem in the case of high dimensional parameter spaces making
 use of present cohort-data.
 
\end_layout

\begin_layout Standard
Regarding it's application to systems biology or medicine a quantification
 of uncertainty of the results is of utmost importance to any practitioner
 facing decisions such as whether to apply a specific treatment or release
 a new drug.
\end_layout

\begin_layout Standard
Although the classical frequentist approach to statistics offers answers
 to this in terms of confidence intervals and hypothesis tests, these techniques
, aiming for point estimates, have a hard time dealing with ill-posed problems
 incorporating uncertainties and unidentifiabilities, often appearing in
 applications with large nonlinear models.
\end_layout

\begin_layout Standard
We will henceforth adopt the Bayesian view which, contrary to the point
 estimates in the frequentist approach, naturally confines the treatment
 of uncertainty by its description in terms of distributions.
\end_layout

\begin_layout Standard
As we will see it also allows for a natural incorporation of data, a circumstanc
e of ever greater importance in a time of progressing digitalization of
 our lives, the medicine and the sciences coining the term big data.
\end_layout

\begin_layout Standard
We will furthermore tackle one of the main points of criticism on the Bayesian
 approach, namely its subjectivity in the choice of the prior: Two scientists,
 given the same data and working with the same model, can come up with different
 results (the posterior) imposing different a priori knowledge about the
 parameters in questions (the prior).
\end_layout

\begin_layout Standard
This critique lead to the school of objective Bayesian analysis (c.f.
 
\begin_inset CommandInset citation
LatexCommand cite
key "berger2006case"

\end_inset

) with the goal to provide methods ensuring consistent results in repeated
 usage by scientists.
\end_layout

\begin_layout Standard
One approach is the choice of so called 
\emph on
non-informative priors
\emph default
, using information theoretic considerations to formalize the notion of
 non-informativity, giving rise to the 
\emph on
Jeffrey's prior
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "jeffreys1946invariant"

\end_inset

 and it's generalization to more general spaces, the
\emph on
 reference priors
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "bernardo1979reference"

\end_inset

.
\end_layout

\begin_layout Standard
An alternative approach is given by the empirical Bayes methods:
\end_layout

\begin_layout Quote
The empirical Bayes approach to statistical decision problems is applicable
 when the same decision problem presents itself repeatedly and independently
 with a fixed but unknown a priori distribution of the parameter - Herbert
 Robbins 
\begin_inset CommandInset citation
LatexCommand cite
key "robbins1964empirical"

\end_inset

.
\end_layout

\begin_layout Standard
Here repeated measurements of individuals from the population in question,
 called cohort data, are used to construct a prior representing that population.
 The first major contribution was by Robbins
\begin_inset CommandInset citation
LatexCommand cite
key "robbins1956"

\end_inset

, deriving explicit formulas for the Bayes estimators of nonparametric priors
 for different families of likelihood functions giving rise to the 
\emph on
nonparametric maximum likelihood estimator
\emph default
 (NPMLE).
 For a while these ideas got largely neglected, probably due to its computationa
l cost, until they was brought back to attention in the parametric case
 by Efron and Morris 
\begin_inset CommandInset citation
LatexCommand cite
key "efron1973stein"

\end_inset

 in 1973.
\end_layout

\begin_layout Standard
Unfortunately the NPMLE approach applied to finite data results in discrete
 distributions.
 In Section 3 we will therefore discuss its regularization.
 We start by treating the topic of penalization of unsmooth solution, leading
 to the 
\emph on
maximal penalized likelihood estimator
\emph default
 (MPLE), and will set this into relation with Bayesian hyperpriors, i.e.
 prior assumption on the prior.
 
\end_layout

\begin_layout Standard
We then continue by introducing the 
\emph on
mutual information penalty
\emph default
, a specific choice for penalizing unsmooth priors based on information
 theoretic considerations.
 We end up with a nonparametric, hence generally applicable, method combining
 the assumptions of least information from the non-informative priors with
 the usage of cohort data from the empirical Bayes approach combining their
 strengths.
 The suggested prior can as well be seen as a generalization of the reference
 priors to the cohort data regime.
 
\end_layout

\begin_layout Standard
We additionally discuss the 
\emph on
doubly-smoothed maximum likelihood estimator
\emph default
 (DS-MLE) estimate by Seo and Lindsay 
\begin_inset CommandInset citation
LatexCommand cite
key "seo2013universally"

\end_inset

, an alternative approach with the idea of tackling the problem of finite
 data at its root by smoothing the data itself followed by the NPMLE.
\end_layout

\begin_layout Standard
In section 4 we will then address the question of how to compute the introduced
 prior estimates numerically.
 We will work with a sample driven Monte Carlo approach, expressing the
 priors as importance samplings.
 We then derive the explicit formulas for the EM algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "dempster1977maximum"

\end_inset

 for the approximation of the NPMLE and the DS-MLE and provide the Jacobian
 for the optimization of the MPLE with the 
\emph on
mutual information penalty
\emph default
.
\end_layout

\begin_layout Standard
We conclude the section with a digression to Markov Chain Monte Carlo sampling
 used for the computation of the Monte Carlo discretization.
\end_layout

\begin_layout Standard
Finally we will apply the developed methods and algorithms to a high-dimensional
 model from systems biology and discuss the results in section 5.
\end_layout

\begin_layout Standard
The essence of this thesis, including the general presentation and lots
 of the notation, originated in close relation to the articles 
\begin_inset CommandInset citation
LatexCommand cite
key "klebanov2016theory,klebanov2016sysmed"

\end_inset

, to which we hence refer for further reading.
\end_layout

\begin_layout Section
Empirical Bayes Methods
\end_layout

\begin_layout Subsection
The Bayesian formalism
\end_layout

\begin_layout Standard
We start by laying out the basic formal tools in the Bayesian setting.
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:condprob"

\end_inset

Let 
\begin_inset Formula $X:\Omega\rightarrow\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $Y:\Omega\rightarrow\mathcal{Y}$
\end_inset

 denote continuous random variables with joint probability density 
\begin_inset Formula $\rho_{X,Y}\left(x,y\right)$
\end_inset

.
 The 
\emph on
conditional probability
\emph default
 density of 
\begin_inset Formula $Y$
\end_inset

 given the event 
\begin_inset Formula $X=x$
\end_inset

 (i.e.
 
\begin_inset Formula $\rho_{X}\left(x\right)>0$
\end_inset

) 
\begin_inset Formula $X$
\end_inset

 is defined as
\begin_inset Formula 
\begin{equation}
\rho_{Y\mid X}\left(y|x\right):=\frac{\rho_{X,Y}\left(x,y\right)}{\rho_{X}\left(x\right)},\label{eq:condprob}
\end{equation}

\end_inset

where 
\begin_inset Formula $\rho_{X}\left(x\right)$
\end_inset

 is the 
\emph on
marginal density
\emph default
 of 
\begin_inset Formula $X$
\end_inset

, i.e.
 the joint density 
\begin_inset Formula $\rho\left(x,y\right)$
\end_inset

 marginalized over all possible 
\begin_inset Formula $y$
\end_inset

:
\begin_inset Formula 
\begin{equation}
\rho_{X}\left(x\right):=\int_{y}\rho\left(x,y\right)\mathrm{d}y.\label{eq:margdens}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Throughout this thesis, we will slightly abuse notation and not distinguish
 between a probability distribution 
\begin_inset Formula $X$
\end_inset

 and its density 
\begin_inset Formula $\rho_{X}$
\end_inset

.
 We will furthermore, whenever conditioning on an event 
\begin_inset Formula $X=x$
\end_inset

 directly denote the condition in the subscript, i.e.
\begin_inset Formula 
\[
\rho_{Y\mid x}\left(y\right):=\rho_{Y\mid X}\left(y\mid x\right),
\]

\end_inset

whenever the corresponding random variable is clear from the context.
\end_layout

\begin_layout Standard
The above equations already imply the heart of Bayesian statistics, 
\emph on
Bayes' theorem:
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 be as above.
 For all 
\begin_inset Formula $y\in\mathcal{Y}$
\end_inset

, such that 
\begin_inset Formula $\rho_{Y}\left(y\right)>0$
\end_inset

 holds: 
\begin_inset Formula 
\begin{equation}
\rho_{X\mid y}\left(x\right)=\frac{\rho_{Y\mid x}\left(y\right)\rho_{X}\left(x\right)}{\rho_{Y}\left(y\right)}=\frac{\rho_{Y\mid x}\left(y\right)\rho_{X}\left(x\right)}{\int_{\mathcal{X}}\rho_{Y\mid x'}\left(y\right)\rho_{X}\left(x'\right)\mathrm{d}x'}.\label{eq:bayes}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
This follows by succesive insertsion of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:condprob"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:margdens"

\end_inset

:
\begin_inset Formula 
\[
\rho_{X\mid y}\left(x\right):\overset{\eqref{eq:condprob}}{=}\frac{\rho_{X,Y}\left(x,y\right)}{\rho_{Y}\left(y\right)}\overset{\eqref{eq:condprob}}{=}\frac{\rho_{Y\mid x}\left(y\right)\rho_{X}\left(x\right)}{\rho_{Y}\left(y\right)}\overset{\eqref{eq:margdens}}{=}\frac{\rho_{Y\mid x}\left(y\right)\rho_{X}\left(x\right)}{\int_{\mathcal{X}}\rho_{Y\mid x'}\left(y\right)\rho_{X}\left(x'\right)\mathrm{d}x'}.
\]

\end_inset


\end_layout

\begin_layout Standard
This formula, obtained by succesive insertion of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:condprob"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:margdens"

\end_inset

, constitutes the heart of Bayesian statistics.
 It tells us how to reconstruct the 
\emph on
posterior distribution
\emph default
 
\begin_inset Formula $\rho_{X\mid y}$
\end_inset

 of the unknown parameter 
\begin_inset Formula $x$
\end_inset

 given data 
\begin_inset Formula $y$
\end_inset

, using the 
\emph on
likelihood
\emph default
 
\begin_inset Formula $\rho_{Y\mid x}\left(y\right)$
\end_inset

 of 
\begin_inset Formula $x$
\end_inset

 given 
\begin_inset Formula $y$
\end_inset

 as well as the 
\emph on
prior
\emph default
 
\begin_inset Formula $\rho_{X}\left(x\right)$
\end_inset

 reflecting our prior assumptions on the density of 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Standard
Note that for fixed 
\begin_inset Formula $y$
\end_inset

 the posterior 
\begin_inset Formula $\rho_{X\mid y}$
\end_inset

 and prior 
\begin_inset Formula $\rho_{X}$
\end_inset

 both are probability densities in 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 The likelihood on the other hand is not a probability density in 
\begin_inset Formula $\mathcal{X}$
\end_inset

 (it is one one in 
\begin_inset Formula $\mathcal{Y}$
\end_inset

 for fixed 
\begin_inset Formula $x$
\end_inset

).
 For this reason it is also called the 
\emph on
likelihood function 
\begin_inset Formula 
\[
L\left(x\mid y\right):=\rho_{Y\mid x}\left(y\right).
\]

\end_inset


\end_layout

\begin_layout Subsection
The likelihood model
\end_layout

\begin_layout Standard
Our inference bases on the combination of a deterministic physical model,
 our description of the reality, with a stochastic measurement error and
 the formalism of Bayes'.
 
\end_layout

\begin_layout Definition
The
\emph on
 physical model
\emph default
 is a map 
\begin_inset Formula $\Phi:\mathcal{X}\subseteq\mathbb{R}^{n}\rightarrow\mathcal{Y}\subseteq\mathbb{R}^{m}$
\end_inset

, mapping some 
\emph on
parameter
\emph default
 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

 to a resulting 
\emph on
state
\emph default
 
\begin_inset Formula $y\in\mathcal{Y}$
\end_inset

.
 The 
\emph on
parameters
\emph default
 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

 are distributed according to some prior 
\begin_inset Formula $X:\Omega\rightarrow\mathcal{X}\sim\rho_{X}$
\end_inset

, with 
\begin_inset Formula $\rho_{X}\left(x\right)>0\forall x\in\mathcal{X}.$
\end_inset


\end_layout

\begin_layout Definition
The data generating 
\emph on
measurement process
\emph default
 
\begin_inset Formula $Z:\Omega\rightarrow\mathcal{Z}\subseteq\mathbb{R}^{m}$
\end_inset

 is modeled as an independent Gaussian perturbation with prescribed covariance
 
\begin_inset Formula $\Sigma$
\end_inset

 of the state conditioned on the parameter:
\begin_inset Formula 
\begin{equation}
\rho_{Z\mid X}\left(z\mid x\right)=\Phi\left(x\right)+E,\quad E\overset{}{\sim}\mathcal{N}\left(0,\Sigma\right),\label{eq:gaussmodel}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathcal{\mathcal{N}}\left(y,\,\Sigma\right)$
\end_inset

 denotes the normal distribution with mean 
\begin_inset Formula $y$
\end_inset

 and covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

.
\end_layout

\begin_layout Standard
This 
\emph on
likelihood model 
\emph default
gives us the means to compute the probabilty of measuring a single measurement
 
\begin_inset Formula $z$
\end_inset

, given the underlying parameter 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Standard
Assuming the 
\emph on
prior distribution 
\emph default
on 
\begin_inset Formula $X$
\end_inset

 was known, this would enable us to compute the 
\emph on
posterior 
\begin_inset Formula $\rho_{X\mid z}$
\end_inset

 
\emph default
given some measurement 
\begin_inset Formula $z\in\mathcal{Z}$
\end_inset

 by straightforward application of the Bayes' theorem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:bayes"

\end_inset

.
\end_layout

\begin_layout Remark
Note that whilst this model is extensible to inference based on multiple
 measurements 
\begin_inset Formula $\bm{z}^{M}:=\left(z_{i}\overset{\text{i.i.d.}}{\sim}\rho_{Z\mid x}\left(x\right)\right)_{i=1}^{M}$
\end_inset

 using the product-rule,
\begin_inset Formula 
\[
\rho_{X\mid\bm{z}^{M}}\left(x\right)=\frac{\prod_{m=1}^{M}\rho_{Z\mid x}\left(z_{i}\right)\rho_{X}\left(x\right)}{\prod_{m=1}^{M}\rho_{Z}\left(z_{i}\right)}
\]

\end_inset

the assumption of identically distributed 
\begin_inset Formula $z_{i}$
\end_inset

 implies that they all depend on the same parameter 
\begin_inset Formula $X=x$
\end_inset

.
 This is the right inference if all measurements come from the same realization
 of 
\begin_inset Formula $X$
\end_inset

, e.g.
 the same subject, but is wrong assuming different measurements correspond
 to independent draws from the prior 
\begin_inset Formula $\rho_{X}$
\end_inset

, e.g.
 multiple subjects.
 We will therefore proceed to a more general framework allowing for the
 inference in the latter case.
\end_layout

\begin_layout Subsection
The empirical Bayes model
\end_layout

\begin_layout Standard
Since in general the 
\emph on
prior 
\emph default

\begin_inset Formula $X$
\end_inset

 cannot be assumed to be known, a number of different methods have been
 established for estimating this prior based on empirical cohort data, giving
 rise to so=called 
\emph on
empirical Bayes methods.
 
\emph default
In that context we extend the model by parametrizing the prior 
\begin_inset Formula $\rho_{X}$
\end_inset

 on a hyperparater 
\begin_inset Formula $\Pi$
\end_inset

, the prior on the set of priors if one likes, resulting in the hierarchical
 model 
\begin_inset Formula $\Pi\rightarrow X\rightarrow Z$
\end_inset

, which we will refer to as the 
\emph on
hyperparametric model
\emph default
.
 
\end_layout

\begin_layout Standard
Most literature confines itself to (finite dimensional) parametric empirical
 Bayes methods, characterized by considering parametrized families of distributi
ons for the priors, e.g.
 
\begin_inset Formula 
\begin{align*}
 & \pi\in\Pi:\Omega\rightarrow=\mathbb{R}^{k},\\
 & \rho_{X\mid\pi}\sim\mathcal{N}\left(\pi,\,I\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
since these may admit explicit formulas for prior point estimates if the
 likelihood model and prior families admit simple forms, as well as circumventin
g regularization issues.
 We aim at a more general solution to the inference problem by allowing
 arbitrary distributions as priors, i.e.
 
\begin_inset Formula 
\begin{align*}
 & ,\\
 & \Pi:\Omega\rightarrow\mathcal{M}_{1}\left(\mathcal{X}\right):=\left\{ \rho\in L^{1}\left(\mathcal{X}\right)\mid\rho\ge0,\,\left\Vert \rho\right\Vert _{L^{1}}=1\right\} ,\\
 & \rho_{X\mid\pi}\left(x\right)=\pi\left(x\right)
\end{align*}

\end_inset

resulting in 
\emph on
nonparametric empirical Bayes 
\emph default
methods.
\end_layout

\begin_layout Standard
The marginal likelihood of a prior 
\begin_inset Formula $\Pi=\pi$
\end_inset

 given a single measurement 
\begin_inset Formula $z$
\end_inset

 is then given by 
\begin_inset Formula 
\[
L\left(\pi\mid z\right)=\rho_{Z\mid\pi}\left(z\right)=\int_{\mathcal{X}}\rho_{Z\mid x}\left(z\right)\pi\left(x\right)\mathrm{d}x.
\]

\end_inset


\end_layout

\begin_layout Standard
Since this likelihood, in contrast to the basic Bayesian model above, does
 not depend on a specific realization of the latent variable 
\begin_inset Formula $X$
\end_inset

 anymore, this allows us to handle multiple measurements coming from independent
 samples of 
\begin_inset Formula $X$
\end_inset

 correctly using the product distribution:
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:defL"

\end_inset

For finite data 
\begin_inset Formula $\bm{z}^{M}=\left(z_{m}\in\mathcal{Z}\right)_{m=1,...,M}$
\end_inset

 we define the likelihood of a prior 
\begin_inset Formula $\pi$
\end_inset

 as
\begin_inset Formula 
\begin{equation}
L\left(\pi\mid\bm{z}^{M}\right):=\prod_{m=1}^{M}\rho_{Z\mid\pi}\left(z_{m}\right)\label{eq:defL}
\end{equation}

\end_inset


\end_layout

\begin_layout Definition
or alternatively in its renormalized logarithmic form as 
\emph on
finite data log-likelihood 
\begin_inset Note Note
status open

\begin_layout Plain Layout
do we need the log/normalization? call above L(pi|z)
\end_layout

\end_inset


\emph default

\begin_inset Formula 
\[
\mathcal{L}\left(\pi\mid\bm{z}^{M}\right):=\frac{1}{M}\log\rho\left(\bm{z}^{M}|\pi\right)=\frac{1}{M}\sum_{m=1}^{M}\log\rho_{Z\mid\pi}\left(z_{m}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
In the case of 
\begin_inset Quotes eld
\end_inset

infinite data
\begin_inset Quotes erd
\end_inset

, represented by the probability density 
\begin_inset Formula $\rho_{Z}$
\end_inset

 of the data-generating random variable 
\begin_inset Formula $Z$
\end_inset

 we define analogously
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:Linf"

\end_inset

Let 
\begin_inset Formula $\rho_{Z}$
\end_inset

 be a probability density on 
\begin_inset Formula $\mathcal{Z}$
\end_inset

.
 The corresponding 
\emph on
infinite data log-likelihood 
\emph default
is defined as
\end_layout

\begin_layout Definition

\emph on
\begin_inset Formula 
\[
\mathcal{L}^{\infty}\left(\pi\mid\rho_{Z}\right):=\int_{\mathcal{Z}}\rho_{Z}\left(z\right)\log\rho_{Z\mid\pi}\left(z\right)\mathrm{d}z.
\]

\end_inset


\end_layout

\begin_layout Standard
This definition, also referred to as the cross entropy between 
\begin_inset Formula $\rho_{Z}$
\end_inset

 and 
\begin_inset Formula $\rho_{Z\mid\pi}$
\end_inset

, follows from the former in the limit for 
\begin_inset Formula $m\rightarrow\infty$
\end_inset

 from the law of large numbers assuming that 
\begin_inset Formula $\rho_{Z}$
\end_inset

 is indeed the data generating distribution:
\begin_inset Formula 
\begin{equation}
z_{m}\overset{i.i.d.}{\sim}\rho_{Z}\Rightarrow\mathcal{L}\left(\pi\mid\bm{z}^{M}\right)\overset{\text{a.s.}}{\underset{M\rightarrow\infty}{\longrightarrow}}\mathcal{L}^{\infty}\left(\pi\mid\rho_{Z}\right).\label{eq:limit}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The following proposition demonstrates how we can recover the data underlying
 
\begin_inset Quotes eld
\end_inset

true prior
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\pi^{*}$
\end_inset

 in the infinite data regime by maximizing the corresponding likelihood
 functional 
\begin_inset Formula $\mathcal{L}^{\infty}.$
\end_inset


\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:identifiable"

\end_inset

Let the hyperparametric model be 
\emph on
well specified
\emph default
, i.e.
 
\begin_inset Formula $\exists\pi^{*}\in\mathcal{M}_{1}\left(\mathcal{X}\right)$
\end_inset

: 
\begin_inset Formula 
\[
\rho_{Z}=\rho_{Z\mid\pi^{*}}
\]

\end_inset

and 
\emph on
identifiable
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
after "chapter 5"
key "van2000asymptotic"

\end_inset

, i.e.
 
\begin_inset Formula $\forall\pi\in\mathcal{M}_{1}:$
\end_inset

 
\begin_inset Formula 
\[
\rho_{Z\mid\pi}=\rho_{Z\mid\pi^{*}}\Rightarrow\pi=\pi^{*}.
\]

\end_inset

Then 
\begin_inset Formula $\pi^{*}$
\end_inset

 is the unique maximizer of 
\begin_inset Formula $\mathcal{L}^{\infty}$
\end_inset


\begin_inset Formula 
\[
\pi^{*}=\underset{\pi\in\mathcal{M}_{1}\left(\mathcal{X}\right)}{\arg\max}\mathcal{L}^{\infty}\left(\pi\right).
\]

\end_inset


\end_layout

\begin_layout Proof
Gibbs' inequality says that
\begin_inset Formula 
\[
\int_{\mathcal{Z}}\rho_{Z}\left(z\right)\log\rho_{Z}\left(z\right)\mathrm{d}z\ge\int_{\mathcal{Z}}\rho_{Z}\left(z\right)\log q\left(z\right)\mathrm{d}z
\]

\end_inset

for any probability density 
\begin_inset Formula $q$
\end_inset

, with equality if and only if 
\begin_inset Formula $q=\rho_{Z}$
\end_inset

.
 The claim follows from the assumptions.
\end_layout

\begin_layout Standard
Both assumptions arise rather naturally.
 If the model is not well specified this merely means the measured data
 
\begin_inset Formula $\rho_{Z}$
\end_inset

 cannot be explained by any prior, thus resulting in an ill-posed problem.
 If on the other hand the model is not identifiable, which by definition
 corresponds to the injectivity of the marginal likelihood function 
\begin_inset Formula $\rho_{Z\mid\pi}$
\end_inset

 as a function of 
\begin_inset Formula $\pi$
\end_inset

, there exists another 
\begin_inset Formula $\pi'\ne\pi^{*}$
\end_inset

 inducing the same measurement distribution so we cannot hope to recover
 the true prior from the data.
\end_layout

\begin_layout Standard
The latter problem can be retracted through lifting the inference problem
 to equivalence classes of priors leading to the same measurements,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\pi\sim\pi':\iff\left\Vert \rho_{Z\mid\pi}-\rho_{Z\mid\pi'}\right\Vert _{L^{1}\left(\mathcal{Z}\right)}=0,\label{eq:equiv}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
which, in the non-identifiable case, is all we can hope for.
\end_layout

\begin_layout Standard
Though in practice usually only finite data is available, and even though
 the limiting property 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:limit"

\end_inset

 might give hope that the NPMLE estimate 
\begin_inset Formula 
\[
\pi_{ML}:=\underset{\pi'\in\mathcal{M}_{1}\left(\mathcal{X}\right)}{\arg\max}\mathcal{L}\left(\pi\mid\bm{z}^{M}\right)
\]

\end_inset

 might approximate 
\begin_inset Formula $\pi^{*}$
\end_inset

 properly, one can prove 
\begin_inset CommandInset citation
LatexCommand cite
after "Theorem 21"
key "lindsay1995mixture"

\end_inset

 that the maximizer of 
\begin_inset Formula $\mathcal{L}$
\end_inset

 is a discrete distribution with at most 
\begin_inset Formula $M$
\end_inset

 nodes.
\end_layout

\begin_layout Standard
In the field of machine learning this phenomenon, commonly occurring for
 insufficient data, is referred to as 
\emph on
overfitting
\emph default
 and usually approached by regularization techniques, methods to enforce
 more regular and smooth solutions.
\end_layout

\begin_layout Section
Regularization
\end_layout

\begin_layout Standard
To address this problem of irregularity we will introduce two regularization
 methods, the 
\emph on
maximum penalized likelihood estimation
\emph default
 (MPLE), introducing a penalization term to the former optimization problem,
 and the 
\emph on
doubly smoothed maximum likelihood estimation 
\emph default
(DS-MLE), based on applying NPMLE after smoothing the data.
\end_layout

\begin_layout Subsection
Regularization via a penalization term
\end_layout

\begin_layout Standard
For this section, let us fix the data data 
\begin_inset Formula $\bm{z}^{M}$
\end_inset

 in Definition
\begin_inset CommandInset ref
LatexCommand eqref
reference "def:defL"

\end_inset

, denoting 
\begin_inset Formula $L\left(\pi\right):=L\left(\pi\mid\bm{z}^{M}\right)$
\end_inset

.
\end_layout

\begin_layout Definition
For a given 
\emph on
roughness penalty 
\emph default
(or 
\emph on
regularization term
\emph default
) 
\begin_inset Formula $\phi:\mathcal{M}_{1}\rightarrow\mathbb{R}$
\end_inset

, responsible for penalizing unsmooth or unwanted solutions with high values,
 the MPLE estimate 
\begin_inset Formula $\pi_{\phi}$
\end_inset

 admits the form 
\begin_inset Formula 
\begin{equation}
\pi_{\phi}=\underset{\pi}{\arg\max}\log L\left(\pi\right)-\phi\left(\pi\right).\label{eq:mple}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This approach also allows for an interpretation in the context of the Bayesian
 hyperparametric model by identifying the penalty 
\begin_inset Formula $\phi$
\end_inset

 with the hyperprior 
\begin_inset Formula $\rho_{\Pi}$
\end_inset

 on 
\begin_inset Formula $\Pi$
\end_inset

 via 
\begin_inset Formula $\rho_{\Pi}\propto e^{-\phi}$
\end_inset

.
 The posterior for 
\begin_inset Formula $\Pi$
\end_inset

 is then 
\begin_inset Formula 
\[
\rho_{\Pi\mid Z}\propto L\left(\pi\right)e^{-\phi\left(\pi\right)}
\]

\end_inset

and thus the 
\emph on
maximum a posteriori 
\emph default
estimate 
\begin_inset Formula $\pi_{MAP}$
\end_inset

 for the hyperparametric model corresponds to the MPLE estimate:
\begin_inset Formula 
\[
\pi_{MAP}=\underset{\pi}{\arg\max}\,L\left(\pi\right)e^{-\phi\left(\pi\right)}=\underset{\pi}{\arg\max}\log L\left(\pi\right)-\phi\left(\pi\right)=\pi_{\phi}.
\]

\end_inset


\end_layout

\begin_layout Standard
One now might might argue that we started with the question of finding the
 correct prior and just complicated the situation by transferring this problem
 to the question of the correct hyperprior.
 While this may be true we argue that the latter can be tackled from a rather
 abstract, problem independent standpoint, hence leading to a more general
 answer.
\end_layout

\begin_layout Subsection
The mutual-information penalty
\end_layout

\begin_layout Standard
Many of the common penalty functions currently in use, penalizing either
 large amplitudes (e.g.
 ridge regression 
\begin_inset CommandInset citation
LatexCommand cite
after "section 1.6"
key "mclachlan2007algorithm"

\end_inset

) or derivatives (c.f.
 
\begin_inset CommandInset citation
LatexCommand cite
key "good1971nonparametric"

\end_inset

) of the prior, are not invariant under reparametrizations of the parameter
 space 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and are rather ad-hoc without a natural derivation.
 Following a more information-theoretic view Good 
\begin_inset CommandInset citation
LatexCommand cite
key "good1963maximum"

\end_inset

 suggested the use of the differential entropy for the penalty
\begin_inset Formula 
\[
\phi_{H_{X}}\left(\pi\right):=\gamma\int_{\mathcal{X}}\rho_{X\mid\pi}\left(x\right)\log\rho_{X\mid\pi}\left(x\right)\mathrm{d}x,
\]

\end_inset

 with 
\begin_inset Formula $\gamma\in\mathbb{R}^{+}$
\end_inset

, the regularization constant, determining the degree of smoothing due to
 this penalty.
 This prior however is still variant under reparametrizations of 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 This means that if two scientists estimate the prior using equivalent models,
 using different systems of units, they could end up with different estimates.
 Hence this penalty does not rectify the problem of subjectivity in the
 Bayesian method.
 We therefore look for a penalty which is invariant under coordinate transformat
ions.
\end_layout

\begin_layout Standard
Embracing the information theoretic approach we therefore propose the use
 of the 
\emph on
mutual information 
\emph default
instead of the entropy for the penalty:
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:mutinf"

\end_inset

Let 
\begin_inset Formula $A:\,\Omega\rightarrow\mathcal{A}$
\end_inset

 and 
\begin_inset Formula $B:\,\Omega\rightarrow\mathcal{B}$
\end_inset

 be two continuous random variables, 
\begin_inset Formula $\rho_{A,B}$
\end_inset

 their joint probability density and 
\begin_inset Formula $\rho_{A},\,\rho_{B}$
\end_inset

 their respective marginal densities.
 Their 
\emph on
mutual information
\emph default
 is defined as
\begin_inset Formula 
\begin{align*}
\mathcal{I}\left(A;B\right): & =\int_{\mathcal{A}}\int_{\mathcal{B}}\rho_{A,B}\left(a,b\right)\log\left(\frac{\rho_{A,B}\left(a,b\right)}{\rho_{A}\left(a\right)\rho_{B}\left(b\right)}\right)\mathrm{d}a\mathrm{d}b\\
 & =\mathbb{E}_{b\sim B}\left[D_{KL}\left(\rho_{A\mid b}\parallel\rho_{A}\right)\right]\\
 & =H\left(B\right)-H\left(B;A\right),
\end{align*}

\end_inset

with 
\begin_inset Formula $D_{KL}$
\end_inset

 being the 
\emph on
Kullback-Leibler divergence 
\emph default
from 
\begin_inset Formula $\rho_{A}$
\end_inset

 to 
\begin_inset Formula $\rho_{A\mid b}$
\end_inset

 
\begin_inset Formula 
\[
D_{KL}\left(\rho_{A\mid b}\parallel\rho_{A}\right):=\int_{\mathcal{A}}\rho_{A\mid b}\left(a\right)\log\frac{\rho_{A\mid b}\left(a\right)}{\rho_{A}\left(a\right)}\mathrm{d}a,
\]

\end_inset


\end_layout

\begin_layout Definition
and 
\begin_inset Formula $H\left(B\right)$
\end_inset

, 
\series bold

\begin_inset Formula $H\left(B;A\right)$
\end_inset

 
\series default
being the 
\emph on
differential entropy
\emph default
 of 
\begin_inset Formula $B$
\end_inset

 respectively the 
\emph on
conditional differential
\emph default
 entropy of 
\begin_inset Formula $B$
\end_inset

 given 
\begin_inset Formula $A$
\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
H\left(B\right) & := & -\int_{\mathcal{B}}\rho_{B}\left(b\right)\log\left(\rho_{B}\right)\mathrm{d}b\\
H\left(B;A\right) & := & \int_{\mathcal{A}}\rho_{A}\left(a\right)H\left(B\mid a\right)\mathrm{d}a\\
 & = & -\int_{\mathcal{A}}\rho_{A}\left(a\right)\int_{\mathcal{B}}\rho_{B\mid a}\left(b\right)\log\left(\rho_{B\mid a}\right)\mathrm{d}b\mathrm{d}a.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The mutual information quantifies the 
\begin_inset Quotes eld
\end_inset

amount of information
\begin_inset Quotes erd
\end_inset

 that one random variable shares with the respective other, expressed by
 the information content of the their joint distribution (
\begin_inset Formula $\rho_{A,B}$
\end_inset

) relative to their joint distribution if they were independent (
\begin_inset Formula $\rho_{A}\rho_{B})$
\end_inset

, weighted by their joint distribution.
\end_layout

\begin_layout Standard
We can gain further insights into its meaning by expressing it in terms
 of another fundamental information-theoretic quantity, the Kullback-Leibler
 divergence 
\begin_inset Formula $D_{KL}\left(A\parallel B\right)$
\end_inset

 from 
\begin_inset Formula $B$
\end_inset

 to 
\begin_inset Formula $A$
\end_inset

 (also called 
\emph on
information gain 
\emph default
or 
\emph on
relative entropy
\emph default
).
 It is a measure for the loss of information when considering 
\begin_inset Formula $B$
\end_inset

 as an approximation to 
\begin_inset Formula $A$
\end_inset

, or consequently in the Bayesian context it is the gain of information
 revising one's beliefs from the prior 
\begin_inset Formula $B$
\end_inset

 to the posterior 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Standard
Hence the mutual information of 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 corresponds to the expected information gain from the prior to the posterior
 over 
\begin_inset Formula $A$
\end_inset

 when the measurements are 
\begin_inset Formula $B$
\end_inset

-distributed.
 This interpretation gives rise to the following definition:
\end_layout

\begin_layout Definition
For 
\begin_inset Formula $\gamma>0$
\end_inset

 constant we define the
\emph on
 mutual information penalty
\emph default
 
\begin_inset Formula 
\begin{eqnarray}
\phi_{I}^{\gamma}\left(\pi\right): & = & -\gamma\mathcal{I}\left(X\mid\pi;Z\mid\pi\right)\nonumber \\
 & = & -\gamma\int_{\mathcal{X}}\rho_{X\mid\pi}\left(x\right)\int_{\mathcal{Z}}\rho_{Z\mid x}\left(z\right)\log\left(\frac{\rho_{Z\mid x}\left(z\right)}{\rho_{Z\mid\pi}\left(z\right)}\right)\mathrm{d}z\mathrm{dx}.\label{eq:ipen}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Minimizing this penalty then corresponds to maximizing the amount of shared
 information between the prior prediction 
\begin_inset Formula $Z\mid\pi$
\end_inset

 and the prior 
\begin_inset Formula $X\mid\pi$
\end_inset

 itself (where we understand these random variables as restrictions of 
\begin_inset Formula $Z$
\end_inset

 respectively 
\begin_inset Formula $X$
\end_inset

 onto the event 
\begin_inset Formula $\Pi=\pi$
\end_inset

).
 In terms of the Kullback-Leibler formulation this means priors 
\begin_inset Formula $\pi$
\end_inset

 are rewarded by the amount of information gain expected from their hypothetical
ly induced measurements, hence encoding a notion of non-informativity.
\end_layout

\begin_layout Standard
Fortunately the mutual information is furthermore transformation invariant,
 thus allowing for the application of the mutual information penalty independent
 of the models parametrization:
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:invariance"

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

, 
\series bold

\begin_inset Formula $Y$
\end_inset


\series default
 be two random variables and 
\begin_inset Formula $\varphi^{-1}:\,\mathcal{X}\rightarrow\tilde{\mathcal{X}}$
\end_inset

, 
\begin_inset Formula $\psi^{-1}:\,\mathcal{Z}\rightarrow\tilde{\mathcal{Z}}$
\end_inset

 be diffeomorphisms defining coordinate transformations and corresponding
 transformed random variables 
\begin_inset Formula $\tilde{X},\,\tilde{Z}$
\end_inset

 with the densities
\begin_inset Formula 
\[
\rho_{\tilde{X},\tilde{Z}}\left(\tilde{x},\tilde{z}\right):=\rho_{X,Z}\left(\varphi\left(\tilde{x}\right),\psi\left(\tilde{z}\right)\right)\left|D\varphi\left(\tilde{x}\right)\right|\left|D\psi\left(\tilde{z}\right)\right|
\]

\end_inset


\begin_inset Formula 
\[
\rho_{\tilde{X}}\left(\tilde{x}\right):=\rho_{X}\left(\varphi\left(\tilde{x}\right)\right)\left|D\varphi\left(\tilde{x}\right)\right|,\,\quad\rho_{\tilde{Z}}\left(\tilde{z}\right):=\rho_{Z}\left(\psi\left(\tilde{z}\right)\right)\left|D\psi\left(\tilde{z}\right)\right|.
\]

\end_inset


\end_layout

\begin_layout Lemma
Let 
\begin_inset Formula $\tilde{\pi}\left(\tilde{x}\right):=\pi\left(\varphi\left(\tilde{x}\right)\right)\left|D\varphi\left(\tilde{x}\right)\right|$
\end_inset

 define the corresponding pullback onto 
\begin_inset Formula $\pi$
\end_inset

 and
\begin_inset Newline newline
\end_inset


\begin_inset Formula $\tilde{\bm{z}}^{M}:=\left(\tilde{z}_{m}\right)_{m=1}^{M},\,\tilde{z}_{m}=\psi^{-1}\left(z_{m}\right),\,m=1,...,M$
\end_inset

 the transformed data.
 
\end_layout

\begin_layout Enumerate
The mutual information 
\begin_inset Formula $\mathcal{I}\left(X;Z\right)$
\end_inset

 is invariant under these coordinate transformations of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\mathcal{Z}$
\end_inset

:
\begin_inset Formula 
\[
\mathcal{I}\left(X;Z\right)=\mathcal{I}(\tilde{X};\tilde{Z}).
\]

\end_inset


\end_layout

\begin_layout Enumerate
The likelihood 
\begin_inset Formula $L\left(\pi\mid\bm{z}^{M}\right)$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:defL"

\end_inset

 is invariant under these coordinate transformations up to a constant factor
 
\begin_inset Formula $c=\prod_{m=1}^{M}\left|D\psi\left(\tilde{\bm{z}}_{m}\right)\right|$
\end_inset

.
\begin_inset Formula 
\[
L\left(\pi\mid\bm{z}^{M}\right)=cL\left(\tilde{\pi}\mid\tilde{\bm{z}}^{M}\right)
\]

\end_inset


\end_layout

\begin_layout Proof
According to the change of variables formula 
\begin_inset Formula 
\begin{align*}
 & \ \mathcal{I}\left(X;Z\right)\\
= & \int_{\mathcal{Z}}\int_{\mathcal{X}}\rho_{X,Z}\left(x,z\right)\log\left(\frac{\rho_{X,Z}\left(x,z\right)}{\rho_{X}\left(x\right)\rho_{z}\left(z\right)}\right)\mathrm{d}x\mathrm{d}z.\\
= & \int_{\mathcal{\tilde{Z}}}\int_{\tilde{X}}\rho_{X,Z}\left(\varphi\left(\tilde{x}\right),\psi\left(\tilde{z}\right)\right)\log\left(\frac{\rho_{X,Z}\left(\varphi\left(\tilde{x}\right),\psi\left(\tilde{z}\right)\right)}{\rho_{X}\left(\varphi\left(\tilde{x}\right)\right)\rho_{Z}\left(\psi\left(\tilde{z}\right)\right)}\right)\left|D\varphi\left(\tilde{x}\right)\right|\left|D\psi\left(\tilde{z}\right)\right|\mathrm{d}\tilde{x}\mathrm{d}\tilde{z}\\
= & \int_{\mathcal{\tilde{Z}}}\int_{\tilde{X}}\rho_{\tilde{X},\tilde{Z}}\left(\tilde{x},\tilde{z}\right)\log\left(\frac{\rho_{\tilde{X},\tilde{Z}}\left(\tilde{x},\tilde{z}\right)}{\rho_{\tilde{X}}\left(\tilde{x}\right)\rho_{\tilde{Z}}\left(\tilde{z}\right)}\right)\mathrm{d}\tilde{x}\mathrm{d}\tilde{z}\\
= & \ \mathcal{I}(\tilde{X};\tilde{Z}).\\
\text{Analogously}\\
 & L\left(\pi\mid\bm{z}^{M}\right)\\
= & \prod_{m=1}^{M}\int_{\mathcal{X}}\rho_{Z\mid x}\left(z_{m}\right)\pi\left(x\right)\mathrm{d}x\\
= & \prod_{m=1}^{M}\int_{\mathcal{X}}\rho_{Z\mid\tilde{x}}\left(\tilde{z}_{m}\right)\pi\left(\varphi\left(\tilde{x}\right)\right)\left|D\varphi\left(\tilde{x}\right)\right|\left|D\psi\left(\tilde{\bm{z}}_{m}\right)\right|\mathrm{d}\tilde{x}\\
= & \prod_{m=1}^{M}\left|D\psi\left(\tilde{\bm{z}}_{m}\right)\right|\int_{\mathcal{X}}\rho_{Z\mid\tilde{x}}\left(\tilde{z_{m}}\right)\tilde{\pi}\left(\tilde{x}\right)\mathrm{d}\tilde{x}\\
= & cL\left(\tilde{\pi}\mid\tilde{\bm{z}}^{M}\right).\\
\end{align*}

\end_inset


\end_layout

\begin_layout Corollary
The maximum penalized likelihood estimator 
\begin_inset Formula $\pi_{\phi_{I}^{\gamma}}$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mple"

\end_inset

 with the mutual information penalty 
\begin_inset Formula $\phi_{I}^{\gamma}$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ipen"

\end_inset

 is invariant under the transformations of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\mathcal{Y}$
\end_inset

 as specified in Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:invariance"

\end_inset

.
\end_layout

\begin_layout Proof
This follow immediately from the above Lemma, recognizing that the multiplicativ
e constant turns into an additive shift due to the logarithm, which does
 not change the 
\begin_inset Formula $\arg\max$
\end_inset

.
\end_layout

\begin_layout Standard
In the case of an additive measurement error we can simplify the MPLE estimator
 by only computing the entropy of the prior predictive distribution:
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:addmeastheo"

\end_inset

For models with additive measurement error 
\begin_inset Formula $E\sim\rho_{E}$
\end_inset


\begin_inset Formula 
\[
Z=\Phi\left(X\right)+E
\]

\end_inset

the MPLE estimate 
\begin_inset Formula $\pi_{\phi_{H_{Z}}^{\gamma}}$
\end_inset

, penalized by the 
\emph on
Z-entropy
\emph default

\begin_inset Formula 
\[
\phi_{H_{Z}}^{\gamma}\left(\pi\right):=-\gamma H\left(Z\mid\pi\right),
\]

\end_inset

 and the mutual information penalty coincide:
\begin_inset Formula 
\[
\pi_{\phi_{H_{Z}}^{\gamma}}=\pi_{\phi_{I}^{\gamma}}.
\]

\end_inset


\end_layout

\begin_layout Proof
In the case of additive measurement error 
\begin_inset Formula $\rho_{Z\mid x}$
\end_inset

 consists of shifts of 
\begin_inset Formula $\rho_{E}$
\end_inset


\begin_inset Formula 
\[
\rho_{Z\mid x}\left(z\right)=\rho_{E}\left(z-\Phi\left(x\right)\right)
\]

\end_inset

and thus 
\begin_inset Formula 
\begin{align*}
H\left(Z\mid x\right) & =\int_{\mathcal{Z}}\rho_{Z\mid x}\left(z\right)\log\left(\rho_{Z\mid x}\left(z\right)\right)\mathrm{d}z\\
 & =\int_{\mathcal{Z}}\rho_{E}\left(z\right)\log\left(\rho_{E}\left(z\right)\right)\mathrm{d}z\\
 & =H\left(E\right).
\end{align*}

\end_inset

Hence the conditional entropy part is constant and both penalties agree
 up to an additive constant
\begin_inset Formula 
\begin{align*}
\phi_{I}^{\gamma}\left(\pi\right) & =-\gamma\left(H\left(Z\mid\pi\right)-H\left(Z;X\mid\pi\right)\right)\\
 & =-\gamma\left(H\left(Z\mid\pi\right)-\int_{\mathcal{X}}\rho_{X\mid\pi}\left(x\right)H\left(Z\mid x\right)\mathrm{d}x\right)\\
 & =-\gamma\left(H\left(Z\mid\pi\right)+H\left(E\right)\right)\\
 & =\phi_{H_{Z}}^{\gamma}+\gamma H\left(E\right).
\end{align*}

\end_inset

Therefore their maxima agree and the estimates are the same.
\end_layout

\begin_layout Remark
For the mentioned additive error model and discrete parameter space 
\begin_inset Formula $\mathcal{X}$
\end_inset

, Klebanov et al.
 
\begin_inset CommandInset citation
LatexCommand cite
after "3.1"
key "klebanov2016theory"

\end_inset

 show that the hyperprior 
\begin_inset Formula $\rho_{\Pi}\left(\pi\right):\propto\exp H\left(Z\mid\pi\right)$
\end_inset

 maximizes the total entropy 
\begin_inset Formula $H\left(Z,\Pi\right)$
\end_inset

 of the whole model and furthermore conjecture this to hold as well for
 continious 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 This derivation from a maximum entropy principle is a further justification
 for the choice of 
\begin_inset Formula $\phi_{H_{Z}}^{\gamma}$
\end_inset

 as a meaningful penalty.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
In the case of no data 
\begin_inset Formula $\bm{z}^{M}$
\end_inset

 the log-likelihood term of 
\begin_inset Formula $\pi_{\phi_{I}^{\gamma}}$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mple"

\end_inset

 vanishes.
 It then matches the definition of reference priors 
\begin_inset CommandInset citation
LatexCommand cite
key "berger2009formal"

\end_inset

 by Beger et al.
 Therefore this estimation routine can be seen as an extension of reference
 priors from a purely non-informative to a cohort-data based empirical Bayes
 approach.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
It can be shown that the mutual information penalty is convex in 
\begin_inset Formula $\pi$
\end_inset

, with strict convexity in the identifiable case, as well as that the likelihood
 function 
\begin_inset Formula $L$
\end_inset

 is convex
\begin_inset CommandInset citation
LatexCommand cite
key "klebanov2016theory"

\end_inset

.
 The MPLE 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mple"

\end_inset

 becomes a concave optimization problem for 
\begin_inset Formula $\phi=\phi_{I}^{\gamma}$
\end_inset

.
\end_layout

\begin_layout Subsection
Regularization by smoothing of the data
\begin_inset CommandInset label
LatexCommand label
name "subsec:dsmle"

\end_inset


\end_layout

\begin_layout Standard
Instead of relying on the coarse approximation of the data generating distributi
on by the empirical distribution 
\begin_inset Formula $\rho_{\bm{z}^{M}}:=\frac{1}{M}\sum_{m=1}^{M}\delta_{z_{m}}\approx\rho_{Z}$
\end_inset

 and then penalizing overconfident priors, we may as well address the issue
 of overfitting by using a smooth approximation 
\begin_inset Formula $\tilde{\rho}_{\bm{z}^{M}}$
\end_inset

 to 
\begin_inset Formula $\rho_{\bm{z}^{M}}$
\end_inset

.
\end_layout

\begin_layout Standard
In the following we will apply the idea of smoothing the data by a kernel
 convolution, introduced by Seo and Lindsay 
\begin_inset CommandInset citation
LatexCommand cite
key "seo2013universally"

\end_inset

 as the DS-MLE, to the empirical Bayes setting.
 
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $K:\mathcal{Z}\rightarrow\mathbb{R}$
\end_inset

 be a kernel density function (i.e.
 
\begin_inset Formula $K\ge0,\,\int_{\mathcal{Z}}K\left(z\right)\mathrm{d}z=1$
\end_inset

) and 
\begin_inset Formula $\bm{z}^{M}=\left(z_{m}\overset{i.i.d.}{\sim}\rho_{Z}\right)_{m=1}^{M}$
\end_inset

 be 
\begin_inset Formula $M$
\end_inset

 indep.
 measurements.
 The 
\emph on
smoothed data density
\emph default
 is then defined as
\begin_inset Formula 
\[
\tilde{\rho}_{\bm{z}^{M}}\left(z\right)=\left(\rho_{\bm{z}^{M}}*K\right)\left(z\right):=\frac{1}{M}\sum_{m=1}^{M}K\left(z-z_{m}\right).
\]

\end_inset


\end_layout

\begin_layout Definition
Note that when smoothing the data we also have to smooth the model (hence
 
\emph on
doubly smoothed
\emph default
) to amount for the additional uncertainty in the data and stay consistent.
\end_layout

\begin_layout Definition
That is in the identifiable case with data-generating prior 
\begin_inset Formula $\pi^{*}$
\end_inset

 (c.f.
 Proposition 
\begin_inset CommandInset ref
LatexCommand ref
reference "prop:identifiable"

\end_inset

) we want
\begin_inset Formula 
\[
\tilde{\rho}_{\bm{z}^{M}}\overset{M\rightarrow\infty}{\longrightarrow}\rho_{Z}*K=:\tilde{\rho}_{Z\mid\pi^{*}}\ne\rho_{Z\mid\pi^{*}},
\]

\end_inset


\end_layout

\begin_layout Definition
to hold.
\end_layout

\begin_layout Definition
Hence the corresponding 
\emph on
smoothed likelihood model
\emph default
 is given by
\begin_inset Formula 
\begin{align*}
\tilde{\rho}_{Z\mid x} & =\rho_{Z\mid x}*K\\
\Rightarrow\tilde{\rho}_{Z\mid\pi} & =\rho_{Z\mid\pi}*K
\end{align*}

\end_inset


\end_layout

\begin_layout Definition
The resulting DS-MLE then takes the form
\begin_inset Formula 
\[
\pi_{DS}:=\underset{\pi\in\mathcal{M}_{1}\left(\mathcal{X}\right)}{\arg\max}\mathcal{\tilde{L}}^{\infty}\left(\pi\mid\tilde{\rho}_{\bm{z}^{M}}\right),
\]

\end_inset

 with 
\begin_inset Formula $\tilde{\mathcal{L}}$
\end_inset

 as in Definition 
\begin_inset CommandInset ref
LatexCommand eqref
reference "def:defL"

\end_inset

 with adjusted likelihood.
\end_layout

\begin_layout Definition
This estimator is proven to be consistent under weak assumptions on the
 kernel and likelihood model (c.f.
 
\begin_inset CommandInset citation
LatexCommand cite
key "seo2013universally"

\end_inset

).
 
\end_layout

\begin_layout Definition
Note however that the choice of a kernel 
\begin_inset Formula $K$
\end_inset

 leaves space for debate.
 Furthermore this procedure is not invariant under reparametrizations of
 the measurement space 
\begin_inset Formula $\mathcal{Z}$
\end_inset

 for fixed kernels.
 Hence this approach, although rather natural and simple does not remedy
 the problem of subjectivity in the Bayesian inference.
\end_layout

\begin_layout Section
Numerical schemes
\end_layout

\begin_layout Subsection
Monte Carlo approximations
\end_layout

\begin_layout Standard
Since the arising integrals are in general not tractable analytically, we
 will make use of sample based discretization of the continuous spaces 
\begin_inset Formula $\mathcal{X}$
\end_inset

, 
\begin_inset Formula $\mathcal{Z}$
\end_inset

 and use Monte Carlo integration for the corresponding integrals.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Enumerate
Given 
\begin_inset Formula $M$
\end_inset

 measurements 
\begin_inset Formula $\bm{z}^{M}=\left(z_{i}\right)_{i=1}^{M}$
\end_inset

 sampled across the population, these are distributed across the marginal
 measurement distribution 
\begin_inset Formula $z_{i}\sim\rho_{Z}$
\end_inset

 by construction of the model.
 We can hence approximate
\begin_inset Formula 
\[
\rho_{Z}\approx\frac{1}{M}\sum_{m=1}^{M}\delta_{z_{m}}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
In the case of the parameter space 
\begin_inset Formula $\mathcal{X}$
\end_inset

 we start with an arbitrary sampling 
\begin_inset Formula $\bm{x=}\left(x_{k}\in\mathcal{X}\right)_{k=1}^{K}$
\end_inset

 distributed according to some density 
\begin_inset Formula $x_{i}\sim\rho_{A}$
\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
nicht p_X
\end_layout

\end_inset

.
 We can now approximate any other density distribution 
\begin_inset Formula $\rho_{B}$
\end_inset

 on 
\begin_inset Formula $\mathcal{X}$
\end_inset

 as an importance sampling with weights 
\begin_inset Formula 
\[
\bm{w}\in\mathcal{W}:=\mathcal{M}_{1}\left(\left\{ 1,...,K\right\} \right)
\]

\end_inset

 such that 
\begin_inset Formula $w_{i}\propto\frac{\rho_{B}\left(x_{i}\right)}{\rho_{A}\left(x_{i}\right)}.$
\end_inset

 We then have
\begin_inset Formula 
\[
\rho_{B}\approx\sum_{k=1}^{K}w_{k}\delta_{x_{k}}.
\]

\end_inset


\end_layout

\begin_layout Standard
Let us express the prior 
\begin_inset Formula $\pi$
\end_inset

 in terms of its weights 
\begin_inset Formula $\bm{w},$
\end_inset


\begin_inset Formula 
\begin{equation}
\pi\approx\rho_{X\mid w}:=\sum_{k=1}^{K}w_{k}\delta_{x_{k}}.\label{eq:priorapprox}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
For ease of notation we will, by slightly abusing it, refer to the discretizatio
n 
\begin_inset Formula $w$
\end_inset

 of a continuous density 
\begin_inset Formula $\pi$
\end_inset

 by its latter name, where appropriate.
 
\end_layout

\begin_layout Standard
We can now approximate the expectation value of any measurable function
 
\begin_inset Formula $g$
\end_inset

 under 
\begin_inset Formula $\pi$
\end_inset

 via 
\begin_inset Formula 
\[
\mathbb{E}_{x\sim\pi}\left[g\left(x\right)\right]=\int_{\mathcal{X}}g\left(x\right)\pi\left(x\right)\mathrm{d}x\approx\sum_{k=1}^{K}w_{k}g\left(x_{k}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
For the prior predictive distribution this leads to
\begin_inset Formula 
\begin{align*}
\rho_{Z\mid\pi}\left(z\right) & =\int_{\mathcal{X}}\rho_{Z\mid x}\left(z\right)\pi\left(x\right)\mathrm{d}x\\
\approx\rho_{Z\mid\bm{w}}\left(z\right) & :=\sum_{k=1}^{K}w_{k}\rho_{Z\mid x_{k}}\left(z\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Inserting this into the marginal likelihood yields
\begin_inset Formula 
\[
L\left(\pi\mid\bm{z}^{M}\right)\approx L\left(\bm{w}\mid\bm{z}^{M}\right):=\prod_{m=1}^{M}\sum_{k=1}^{K}w_{k}\rho_{Z\mid x_{k}}\left(z_{m}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
In order to integrate over the density 
\begin_inset Formula $\rho_{Z\mid\pi}$
\end_inset

 for the entropy hyperprior, we approximate its density by an additional
 weighted sampling consisting of 
\begin_inset Formula $\bar{K}>K$
\end_inset

 
\begin_inset Formula $\mathcal{Z}$
\end_inset

-samples generated from the given 
\begin_inset Formula $\mathcal{X}$
\end_inset

-sampling by
\begin_inset Formula 
\[
\bar{\bm{z}}:=\left(\bar{z}_{j}\sim\rho_{Z\mid x_{J\left(j\right)}}\right)_{j=1}^{\bar{K}}
\]

\end_inset

with corresponding weights
\begin_inset Formula 
\[
\bar{w_{j}}:=\frac{w_{J\left(j\right)}}{\#J^{-1}\left(J\left(j\right)\right)}.
\]

\end_inset

Here 
\begin_inset Formula $J:\{1,2,...,\bar{K}\}\rightarrow\left\{ 1,2,...,K\right\} $
\end_inset

 denotes a surjective index mapping function, mapping from the 
\begin_inset Formula $\mathcal{Z}$
\end_inset

- to the corresponding 
\begin_inset Formula $\mathcal{X}$
\end_inset

-samples indices.
 The normalizing factor in the weights amounts for the inflation by multiple
 
\begin_inset Formula $\mathcal{Z}$
\end_inset

-samples 
\begin_inset Formula $\bar{z}_{i},\,\bar{z}_{j}$
\end_inset

 from a single 
\begin_inset Formula $\mathcal{X}$
\end_inset

-sample in the case of 
\begin_inset Formula $J\left(i\right)=J\left(j\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The Monte Carlo approximation to the 
\begin_inset Formula $\mathcal{Z}$
\end_inset

-entropy then takes the form
\begin_inset Formula 
\begin{align*}
H\left(Z\mid\pi\right) & \approx H\left(Z\mid\bm{w}\right):=-\sum_{j=1}^{\bar{K}}\bar{w}_{j}\log\left(\sum_{k=1}^{K}w_{k}\rho_{Z\mid x_{k}}\left(\bar{z}_{j}\right)\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
EM algorithm
\end_layout

\begin_layout Standard
We will first show how to apply the well known 
\emph on
Expectation Maximization 
\emph default
(EM) algorithm for the NPMLE and DS-MLE.
\end_layout

\begin_layout Standard
Herefore let us first recapitulate the EM-algorithm following the classic
 paper of Dempster, Laird and Rubin 
\begin_inset CommandInset citation
LatexCommand cite
key "dempster1977maximum"

\end_inset

.
\end_layout

\begin_layout Standard
We start by defining the 
\emph on
complete data likelihood functio
\emph default
n
\begin_inset Formula 
\begin{align*}
L^{c}\left(\pi\mid\bm{x},\bm{z}\right): & =\prod_{m=1}^{M}\rho_{X\mid\pi}\left(x_{m}\right)\rho_{Z\mid x_{m}}\left(z_{m}\right),\\
\end{align*}

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
not yet correct
\end_layout

\end_inset

the likelihood of a specific prior represented by 
\begin_inset Formula $w$
\end_inset

 given the measurements 
\begin_inset Formula $\bm{z}=\left(z_{m}\right)_{m=1}^{M}$
\end_inset

 with corresponding parameters 
\begin_inset Formula $\bm{x}=\left(x_{m}\right)_{m=1}^{M}$
\end_inset

, where the completeness is meant in the context of knowing all involved
 variables, including 
\begin_inset Formula $\bm{x}$
\end_inset

.
\end_layout

\begin_layout Standard
Based on this we define the 
\emph on
expected complete data log-likelihood 
\emph default
of 
\begin_inset Formula $\pi$
\end_inset

 as the expectation over the posterior 
\begin_inset Formula $\bm{x}_{n}:=\left(x_{n,m}\overset{\text{ind.}}{\sim}\rho_{X\mid\pi_{n},z_{m}}\right)_{m=1}^{M}$
\end_inset

 under the current estimate 
\begin_inset Formula $\pi_{n}$
\end_inset

 of the logarithm of the complete data likelihood conditioned on 
\begin_inset Formula $\pi$
\end_inset

 and the observed cohort-data 
\begin_inset Formula $\bm{z}^{M}$
\end_inset

:
\emph on

\begin_inset Formula 
\begin{align}
Q\left(\pi\mid\pi_{n}\right):= & \mathbb{E}_{\bm{x}_{n}}\left[\log\left(L^{c}\left(\pi\mid\bm{x}_{n},\bm{z}^{M}\right)\right)\right]\nonumber \\
= & \mathbb{E}_{\bm{x}_{n}}\left[\sum_{m=1}^{m}\log\left(\rho_{X\mid\pi}\left(x_{n,m}\right)\rho_{Z\mid\pi}\left(z_{m}\right)\right)\right]\label{eq:expcomploglike}\\
= & \sum_{m=1}^{M}\mathbb{E}_{x\sim\rho_{X\mid\pi_{n},z_{m}}}\left[\log\left(\rho_{X\mid\pi}\left(x\right)\rho_{Z\mid x}\left(z_{m}\right)\right)\right].\nonumber \\
\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
The second step follows from the insight that after exchanging integration
 and summation the log term for the 
\begin_inset Formula $m$
\end_inset

'ths summand 
\begin_inset Formula $\log$
\end_inset

 term depends only on a single 
\begin_inset Formula $x_{n,m}$
\end_inset

 and thus the other 
\begin_inset Formula $x_{n,m'}$
\end_inset

's, 
\begin_inset Formula $m'\ne m,$
\end_inset

 get marginalized out.
\end_layout

\begin_layout Standard
The EM algorithm works by iteratively maximizing the expected complete-data
 log-likelihood under the current estimate 
\begin_inset Formula $\pi_{n}$
\end_inset

:
\begin_inset Formula 
\[
\pi_{n+1}:=\underset{\pi\in\mathcal{W}}{\arg\max\,}Q\left(\pi\mid\pi_{n}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
For a proof of uniqueness and convergence in the case of strictly convex
 likelihoods we refer to 
\begin_inset CommandInset citation
LatexCommand cite
key "dempster1977maximum"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "wu1983convergence"

\end_inset

.
\end_layout

\begin_layout Standard
Let us compute the corresponding formulas for our Monte Carlo approximations.
\end_layout

\begin_layout Standard
For the approximations 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:priorapprox"

\end_inset

 of 
\begin_inset Formula $\pi$
\end_inset

 and 
\begin_inset Formula $\pi_{n}$
\end_inset

 in terms of 
\begin_inset Formula $\bm{w}$
\end_inset

 and 
\begin_inset Formula $\bm{w}_{n}$
\end_inset

, respectively,
\begin_inset Formula 
\begin{align*}
\pi & \approx\sum_{k=1}^{K}w_{k}\delta_{x_{k}},\\
\pi_{n} & \approx\sum_{k=1}^{K}w_{n,k}\delta_{x_{k}},\\
\end{align*}

\end_inset

we have for any measurable function 
\begin_inset Formula $f$
\end_inset


\begin_inset Formula 
\begin{align*}
\mathbb{E}_{x\sim\rho_{X\mid\pi_{n},z_{m}}}\left[f\left(x\right)\right] & =\int_{\mathcal{X}}\rho_{X\mid\pi_{n},z_{m}}\left(x\right)f\left(x\right)\mathrm{d}x\\
 & \approx\sum_{k=1}^{K}w_{n,k}\frac{\rho_{Z\mid x_{k}}\left(z_{m}\right)}{\rho_{Z\mid\bm{w_{n}}}\left(z_{m}\right)}f\left(x_{k}\right).
\end{align*}

\end_inset

Therefore we can approximate 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:expcomploglike"

\end_inset

 by
\begin_inset Formula 
\begin{align*}
Q\left(\pi\mid\pi_{n}\right)\approx Q\left(\bm{w}\mid\bm{w}_{n}\right) & :=\sum_{m=1}^{M}\sum_{k=1}^{K}w_{n,k}\frac{\rho_{Z\mid x_{k}}\left(z_{m}\right)}{\rho_{Z\mid\bm{w_{n}}}\left(z_{m}\right)}\log\left(\rho_{X\mid\bm{w}}\left(x_{k}\right)\rho_{Z\mid x_{k}}\left(z_{m}\right)\right)\\
 & =\sum_{m=1}^{M}\sum_{k=1}^{K}w_{n,k}\frac{\rho_{Z\mid x_{k}}\left(z_{m}\right)}{\rho_{Z\mid\bm{w_{n}}}\left(z_{m}\right)}\log\left(w_{k}\rho_{Z\mid x_{k}}\left(z_{m}\right)\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can furthermore explicitly compute the maximizer to the arising optimization
 problem
\begin_inset Formula 
\[
\bm{w}_{n+1}:=\underset{\bm{w}\in\mathcal{W}}{\arg\max\,}Q\left(\bm{w}\mid\bm{w}_{n}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
A necessary condition is that the gradient of 
\begin_inset Formula $Q$
\end_inset


\begin_inset Formula $\left(\cdot\mid\bm{w}_{n}\right)$
\end_inset

at 
\begin_inset Formula $\bm{w}$
\end_inset

 is orthogonal to the tangent space 
\begin_inset Formula $T_{\bm{w}}\mathcal{W}$
\end_inset

 of 
\begin_inset Formula $\mathcal{W}$
\end_inset

, which means that all components of the gradient are equal:
\begin_inset Formula 
\begin{align*}
 & \frac{\partial Q\left(\bm{w}\mid\bm{w}_{n}\right)}{\partial w_{k}}\perp T_{\bm{w}}\mathcal{W}\\
\Rightarrow\, & \exists c\in\mathbb{R}\:\forall k=1,...,K:\\
 & \frac{\partial Q\left(\bm{w}\mid\bm{w}_{n}\right)}{\partial w_{k}}=\sum_{m=1}^{M}\frac{w_{n,k}}{w_{k}}\frac{\rho_{Z\mid x_{k}}\left(z_{m}\right)}{\rho_{Z\mid w_{n}}\left(z_{m}\right)}=c\\
\Rightarrow\, & w_{k}=\frac{w_{n,k}}{c}\sum_{m=1}^{M}\frac{\rho_{Z\mid x_{k}}\left(z_{m}\right)}{\rho_{Z\mid w_{n}}\left(z_{m}\right)}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $\sum_{k}w_{k}=1$
\end_inset

 we conclude 
\begin_inset Formula $c=1/M$
\end_inset

 and hence end up with the explicit EM step 
\begin_inset Formula $\bm{w}_{n+1}=\psi\left(\bm{w}_{n}\right):$
\end_inset


\begin_inset Formula 
\begin{equation}
\psi\left(\bm{w}_{n}\right)_{k}:=\frac{w_{n,k}}{M}\sum_{m=1}^{M}\frac{\rho_{Z\mid x_{k}}\left(z_{m}\right)}{\rho_{Z\mid w_{n}}\left(z_{m}\right)}.\label{eq:emstep}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Iterative application of this formula henceforth converges to the weights
 approximating the desired NPMLE estimate 
\begin_inset Formula $\pi_{ML}$
\end_inset

.
\end_layout

\begin_layout Standard
Applying the Monte Carlo discretization to the DS-MLE setting (
\begin_inset CommandInset citation
LatexCommand cite
key "seo2013universally"

\end_inset

) we end up with the same EM-algorithm of the NPMLE applied to the augmented
 data points 
\begin_inset Formula $\left(z_{m}\overset{i.i.d}{\sim}\tilde{\rho}_{\bm{z}^{M}}\right)_{m=1}^{M^{\text{aug }}}$
\end_inset

 and the smoothed likelihoods (c.f.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:dsmle"

\end_inset

).
 Since 
\begin_inset Formula $M$
\end_inset

 data points result in maximally 
\begin_inset Formula $M$
\end_inset

 peaks in the NPMLE 
\begin_inset CommandInset citation
LatexCommand cite
key "lindsay1995mixture"

\end_inset

, a necessary condition for strictly positive weights is that the number
 of augmented data points is at least that of the parameter-space nodes,
 
\begin_inset Formula $M^{\text{aug }}>K.$
\end_inset

 
\end_layout

\begin_layout Subsection
Optimization for MPLE
\end_layout

\begin_layout Standard
In the case of an additive measurement error the Monte Carlo discretized
 version of the MPLE 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mple"

\end_inset

 with the mutual information penalty takes the form
\begin_inset Formula 
\begin{align*}
w_{\phi_{I}^{\gamma}} & =\arg\max_{\bm{w}\in\mathcal{W}}O\left(\bm{w}\right)\\
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
with objective function
\begin_inset Formula 
\begin{align*}
O\left(w\right) & =\log L\left(\bm{w}\mid\bm{z}^{M}\right)+\gamma H\left(Z\mid w\right)\\
 & =\sum_{m=1}^{M}\log\sum_{k=1}^{K}w_{k}\rho_{Z\mid x_{k}}\left(z_{m}\right)-\gamma\sum_{j=1}^{\bar{K}}\bar{w}_{j}\log\left(\sum_{k=1}^{K}w_{k}\rho_{Z\mid x_{k}}\left(\bar{z}_{j}\right)\right).
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Being a composition of linear and concave functions the objective 
\begin_inset Formula $O$
\end_inset

 is easily shown to be concave, allowing for the use of constraint concave/conve
x optimization routines.
\end_layout

\begin_layout Standard
The high dimensionality of 
\begin_inset Formula $\bm{w}$
\end_inset

 suggests the usage of derivative based optimization techniques.
 The derivative of the objective is given by
\begin_inset Formula 
\begin{align*}
\frac{\partial O}{\partial w_{k}}\left(w\right) & =\sum_{m=1}^{M}\frac{\rho_{Z\mid x_{k}}\left(z_{m}\right)}{\sum_{k'=1}^{K}w_{k'}\rho_{Z\mid x_{k'}}\left(z_{m}\right)}\\
 & +\gamma\sum_{j=1}^{\bar{K}}\frac{\bar{w}_{j}\rho_{Z\mid x_{k}}\left(\bar{z}_{j}\right)}{\sum_{k'=1}^{K}w_{k'}\rho_{Z\mid x_{k'}}\left(\bar{z}_{j}\right)}\\
 & +\gamma\sum_{j\in J^{-1}\left(k\right)}\frac{\log\left(\sum_{k'=1}^{K}w_{k'}\rho_{Z\mid x_{k'}}\left(\bar{z}_{j}\right)\right)}{\#J^{-1}\left(k\right)}.
\end{align*}

\end_inset


\end_layout

\begin_layout Remark*
Instead of first discretizing and then deriving, one might as well try to
 work with the discretization of the derivative (of the corresponding continuous
 objective).
 Whilst this promises to better approximate the gradient of the underlying
 continuous problem, the gradient does not fit to the objective anymore
 and may therefore lead to problems with optimization routines relying on
 correctness of the gradient.
\end_layout

\begin_layout Subsection
Markov Chain Monte Carlo
\end_layout

\begin_layout Standard
The quality of the Monte Carlo approximations greatly depends on the choice
 of the importance samples 
\begin_inset Formula $\bm{x}$
\end_inset

.
 Whilst an equidistant grid may work well for low dimensional parameter
 spaces 
\begin_inset Formula $\mathcal{X}$
\end_inset

, its number of samples/gridpoints increases exponentially with the dimension
 of 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 This so called curse of dimensionality suggests the use of Markov Chain
 Monte Carlo (MCMC) sampling methods, a popular sampling scheme for probability
 densities 
\begin_inset Formula $f$
\end_inset

 defined over high-dimensional spaces.
\end_layout

\begin_layout Standard
The basic idea of MCMC methods revolves around constructing an ergodic Markov
 Chain, a stochastic process whose conditional probability for future states
 depends only on the current state, which has the desired target density
 
\begin_inset Formula $f$
\end_inset

 as stationary density.
 In the limit of infinite sample sizes the samples from the Markov Chain
 then are distributed according to 
\begin_inset Formula $f$
\end_inset

.
\end_layout

\begin_layout Standard
The probably most common MCMC scheme is the MetropolisâHastings (MH) algorithm.
 It works by iteratively sampling a proposal 
\begin_inset Formula $x_{n}^{'}\in\mathcal{X}$
\end_inset

 
\begin_inset Quotes eld
\end_inset

around
\begin_inset Quotes erd
\end_inset

 the last MCMC sample 
\begin_inset Formula $x_{n-1}$
\end_inset

 according to a prescribed 
\emph on
proposal density 
\begin_inset Formula $Q\left(x_{n}^{'}\mid x_{n-1}\right)$
\end_inset

 
\emph default
and accepting or rejecting that proposal in such a way that the resulting
 MCMCs stationary distribution is 
\begin_inset Formula $f.$
\end_inset


\end_layout

\begin_layout Standard
Let us define the corresponding Markov process in terms of its 
\emph on
transition probabilities 
\begin_inset Formula $P\left(x_{n+1}\mid x_{n}\right)$
\end_inset


\emph default
, starting from the detailed balance condition 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat*}{2}
 &  & P\left(x'\mid x\right)P\left(x\right) & =P\left(x\mid x'\right)P\left(x'\right)\\
\Leftrightarrow\quad &  & \frac{P\left(x\right)}{P\left(x'\right)} & =\frac{P\left(x\mid x'\right)}{P\left(x'\mid x\right)}\\
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
which ensures the existence of a stationary distribution 
\begin_inset Note Note
status open

\begin_layout Plain Layout
ref
\end_layout

\end_inset


\begin_inset Formula $\pi=P\pi$
\end_inset

.
\end_layout

\begin_layout Standard
Taking the ansatz of splitting the transition probability into a proposal
 distribution 
\begin_inset Formula $g$
\end_inset

 and an acceptance distribution 
\begin_inset Formula $A$
\end_inset


\begin_inset Formula 
\[
P\left(x'\mid x\right)=g\left(x'\mid x\right)A\left(x'\mid x\right),
\]

\end_inset

we end up with 
\begin_inset Formula 
\begin{equation}
\frac{A\left(x'\mid x\right)}{A\left(x\mid x'\right)}=\frac{P\left(x'\right)}{P\left(x\right)}\frac{g\left(x\mid x'\right)}{g\left(x'\mid x\right)}.\label{eq:mhacceptance}
\end{equation}

\end_inset

The Metropolis-Hastings choice for the acceptance distribution 
\begin_inset Formula 
\[
A\left(x'\mid x\right):=\min\left(1,\frac{P\left(x'\right)}{P\left(x\right)}\frac{g\left(x\mid x'\right)}{g\left(x'\mid x\right)}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
satisfies this equation, since either 
\begin_inset Formula $A\left(x'\mid x\right)$
\end_inset

 or 
\begin_inset Formula $A\left(x\mid x'\right)$
\end_inset

 is 
\begin_inset Formula $1$
\end_inset

, while the other equals the desired right hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mhacceptance"

\end_inset

 (or its inverse).
\end_layout

\begin_layout Standard
This choice allows for the formulation of the following theorem.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $f,g\in\mathcal{M}_{1}\left(\mathcal{X}\right)$
\end_inset

 with 
\begin_inset Formula $f\left(x\right)>0,\,g\left(x\right)>0\,\forall x\in\mathcal{X}$
\end_inset

.
 Then the Markov chain defined by 
\begin_inset Formula 
\[
P\left(x'\mid x\right)=g\left(x'\mid x\right)\min\left(1,\frac{f\left(x'\right)}{f\left(x\right)}\frac{g\left(x\mid x'\right)}{g\left(x'\mid x\right)}\right)
\]

\end_inset

admits 
\begin_inset Formula $f$
\end_inset

 as unique stationary distribution 
\begin_inset Formula 
\[
f=Pf.
\]

\end_inset


\end_layout

\begin_layout Proof
The fact that 
\begin_inset Formula $f$
\end_inset

 is indeed a stationary distribution follows by construction, uniqueness
 follows from irreducibility and aperiodicity which is given due to positivity
 of 
\begin_inset Formula $P$
\end_inset

.
\end_layout

\begin_layout Standard
Hence we can use this Markov crocess to sample a Markov Chain according
 to the MH-MCMC algorithm:
\end_layout

\begin_layout Enumerate
Start from an arbitrary point 
\begin_inset Formula $x_{0}$
\end_inset


\end_layout

\begin_layout Enumerate
Sample a proposal state 
\begin_inset Formula $x_{n+1}'\sim g\left(x_{n+1}\mid x_{n}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
With probability 
\begin_inset Formula $A\left(x_{n+1}'\mid x_{n}\right)$
\end_inset

 accept the proposal, i.e.
 set 
\begin_inset Formula $x_{n+1}:=x_{n+1}'$
\end_inset

, otherwise reject it, i.e.
 set 
\begin_inset Formula $x_{n+1}:=x_{n}$
\end_inset


\end_layout

\begin_layout Enumerate
Increase n by 1 and resume with 2 until sufficiently many states were generated
\end_layout

\begin_layout Standard
The speed of convergence to the stationary distribution is strongly influenced
 by the choice of the proposal distribution.
 A common choice for the proposal distribution is the normal distribution
 centered at the current state 
\begin_inset Formula $x_{n}$
\end_inset

, but even here the choice of the covariance is vital for rapid mixing of
 the resulting Markov Chain.
\end_layout

\begin_layout Standard
To circument the obstacle of choosing a proper proposal covariance we decided
 to use the adaptive Metropolis (AM) algorithm by Haario et.
 al 
\begin_inset CommandInset citation
LatexCommand cite
key "haario2001adaptive"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "haario2001adaptive"

\end_inset

 which tunes the proposal covariance matrix online based on the current
 samples, according to
\begin_inset Formula 
\begin{align*}
g_{n}(\text{Â·} & \mid x_{n})=\mathcal{N}(x,\,\Sigma_{0}) & \text{if }n\le2d,\\
g_{n}(\text{Â·} & \mid x_{n})=(1-Î²)\mathcal{N}\left(x,\,\frac{2.38^{2}}{d}\Sigma_{n}\right)+Î²\mathcal{N}(x,\,\Sigma_{0}) & \text{if }n>2d,
\end{align*}

\end_inset

with 
\begin_inset Formula $d$
\end_inset

 being the dimensionality of the sampling space 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\frac{2.38^{2}}{d}$
\end_inset

 a scaling constant considered to be optimal for high dimensions 
\begin_inset CommandInset citation
LatexCommand cite
key "roberts1997weak"

\end_inset

.
 
\begin_inset Formula $\Sigma_{n}$
\end_inset

 is the covariance matrix estimate based on the previous samples 
\begin_inset Formula $\left(x_{i}\right)_{i=0}^{n}$
\end_inset

, which can be computed recursively, and 
\begin_inset Formula $\Sigma_{0}$
\end_inset

 an initially chosen positive definite covariance matrix.
 The linear combination with the fixed normal by 
\begin_inset Formula $0<\beta<1$
\end_inset

 ensures that the resulting proposal covariance stays positive definite
 even in the case of singular 
\begin_inset Formula $\Sigma_{n}$
\end_inset

.
\end_layout

\begin_layout Standard
The acceptance step remains the same as with the standard MH-MCMC.
 This Markov chain still admits the desired target density as stationary
 distribution, assuming it is log-concave outside of some arbitrary region
 (for a proof see 
\begin_inset CommandInset citation
LatexCommand cite
key "Roberts2009"

\end_inset

).
\end_layout

\begin_layout Section
Application
\end_layout

\begin_layout Standard
We will now discuss the application of the developed empirical Bayes method
 on the basis of a high-dimensional ordinary differential equation model
 accompanied by real-life measurements.
 We will therefore introduce the model and obtain a discretization of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 by sampling from the individual Bayesian posteriors by means of MCMC sampling.
 We will then compute the MPMLE, MPLE and DS-MLE prior estimates and discuss
 the results.
 
\end_layout

\begin_layout Subsection
The problem
\end_layout

\begin_layout Standard
Our physical model, consisting of a system of 33 ordinary differential equations
 (ODE), models the feedback mechanisms of the prevalent hormones in the
 female menstrual cycle with a focus on GnRH-receptor binding and was derived
 by RÃ¶blitz et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "roblitz2013"

\end_inset

.
 For the defining equations we refer to the original paper.
\end_layout

\begin_layout Standard
This system is parametrized by 114 parameters, out of which 21 (the Hill
 parameters) are considered fixed for the following survey.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "roblitz2013"

\end_inset

 the authors provide point estimates for the parameters, although concluding
 that only 52 were identifiable, as well as initial conditions.
 We will denote these as 
\emph on
nominal parameters 
\begin_inset Formula $\theta^{\text{nom}}$
\end_inset

 
\emph default
and 
\emph on
initial conditions
\emph default
 
\begin_inset Formula $y_{0}^{nom}$
\end_inset

 and use them as initial conditions for the Markov chain as well as for
 the prior computation.
\end_layout

\begin_layout Standard
Let us denote the forward solution of the ODE at time 
\begin_inset Formula $t$
\end_inset

, given parameters 
\begin_inset Formula $\theta\in\mathbb{R}^{82}$
\end_inset

 and initial conditions 
\begin_inset Formula $y_{0}\in\mathbb{R}^{33}$
\end_inset

, as
\begin_inset Formula 
\[
\phi\left(t;\theta,y_{0}\right)\in\mathbb{R}^{33}.
\]

\end_inset


\end_layout

\begin_layout Standard
Our data consists of blood concentrations of follicle-stimulating hormone
 (FSH), luteinizing hormone (LH), estradiol (E2) and progesterone (P4) measured
 from 53 healthy women over thirty days, roughly every second day.
 This data was collected in the context of PAEON, a collaborative European
 research project on eHealth.
 Denote the set of measurements for a single patient 
\begin_inset Formula $m$
\end_inset


\begin_inset Formula 
\[
z^{m}:=\left(z_{t,i}^{m}\right)_{\left(t,i\right)\in I_{z}^{m}}
\]

\end_inset

 with 
\begin_inset Formula $z_{t,i}^{m}$
\end_inset

 being the single measurement of species 
\begin_inset Formula $i=1,...,4$
\end_inset

 at time 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $I_{z}^{m}$
\end_inset

 denoting the index set of available measurements.
\end_layout

\begin_layout Standard
To impose the condition of periodicity of the data onto our inference process
 we further augment the data by a copy of itself shifted in time by an additiona
l latent parameter representing the period length 
\begin_inset Formula $\tau>0$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
z^{m,\tau}:=\left(\hat{z}_{t,i}^{m}\right)_{\left(t,i\right)\in I_{z}^{m,\tau}},
\]

\end_inset

with augmented index set 
\begin_inset Formula 
\[
I_{z}^{m,\tau}=\bigcup_{\left(t,i\in I_{z}^{m}\right)}\left\{ \left(t,i\right),\left(t+\tau,i\right)\right\} 
\]

\end_inset

 and augmented measurements
\begin_inset Formula 
\[
\hat{z}_{t,i}^{m}:=\begin{cases}
z_{t,i}^{m} & \text{if }\left(t,i\right)\in I_{z}^{m}\\
z_{t-\tau,i}^{m} & \text{otherwise}
\end{cases}.
\]

\end_inset

 
\end_layout

\begin_layout Standard
Subsuming the latent model parameters 
\begin_inset Formula $\theta$
\end_inset

 (
\begin_inset Formula $82)$
\end_inset

, the initial conditions 
\begin_inset Formula $y_{0}$
\end_inset

 
\begin_inset Formula $(33)$
\end_inset

 and the period length 
\begin_inset Formula $\tau$
\end_inset

 we end up with 116 parameters 
\begin_inset Formula 
\[
x=\left(\theta,y_{0},\tau\right)\in X:=\left(\Theta,Y_{0},T\right)
\]

\end_inset

 for the Bayesian inference.
\end_layout

\begin_layout Standard
We model each single as afflicted by a independent Gaussian measurement
 error with independent componentwise standard deviations of 
\begin_inset Formula $10\%$
\end_inset

 of their respective order of magnitude, estimated from the nominal solution
 
\begin_inset Formula $\phi^{nom}\left(t\right):=\phi\left(t;y_{0}^{\text{nom}},\theta^{\text{nom}}\right)$
\end_inset

:
\begin_inset Formula 
\[
\sigma_{1}^{\text{meas}}=12,\,\sigma_{2}^{\text{meas}}=1,\,\sigma_{3}^{\text{meas}}=40,\,\sigma_{4}^{\text{meas}}=1.5.
\]

\end_inset


\end_layout

\begin_layout Standard
We end up with the following likelihood function for the parameters 
\begin_inset Formula $x$
\end_inset

 given a single person's data 
\begin_inset Formula $z_{m}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L\left(x\mid z^{m}\right)=L\left(\theta,y_{0},\tau\mid z^{m}\right)=\prod_{\left(t,i\right)\in I_{z}^{m,\tau}}\left(2\pi\right)^{-\frac{1}{2}}\left(\sigma_{i}^{\text{meas}}\right)^{-2}\exp\left(-\frac{1}{2}\left(\frac{\phi\left(t;\theta,y_{0}\right)_{i}-z_{t,i}^{m,\tau}}{\sigma_{i}^{\text{meas}}}\right)^{2}\right).
\]

\end_inset


\end_layout

\begin_layout Remark
This likelihood correctly reflects the amount of information available dependent
 on the number of measurements.
 A higher number of measurements results in sharper specified likelihood
 function.
 This will allow us to correctly treat different patients/measurements together
 in the upcoming empirical Bayes analysis.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
One might argue that this model manipulates the data and hence is no more
 of the form 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:gaussmodel"

\end_inset

.
 But it is equivalent to the model obtained by just duplicating the data
 and subsuming the shift operation into a new forward solution operator,
 hence Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:addmeastheo"

\end_inset

 still applies.
\end_layout

\begin_layout Standard
For the initial Bayesian sampling procedure, necessary for arrival at a
 discretization of 
\begin_inset Formula $\mathcal{X}$
\end_inset

, we furthermore need to specify an initial prior 
\begin_inset Formula $\pi_{0}$
\end_inset

 on our parameters 
\begin_inset Formula $X:=\left(\theta,y_{0},\tau\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Since we did not want to imply any knowledge on 
\begin_inset Formula $\theta$
\end_inset

 but their order of magnitude we chose the uniform prior bounded by 
\begin_inset Formula $\alpha:=5$
\end_inset

 times the multiple of its corresponding nominal parameter value
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta_{i}\mid\pi_{0}=\mathcal{U}\left(0,\alpha\theta_{i}^{\text{nom}}\right),\quad i=1,...,82.
\]

\end_inset


\end_layout

\begin_layout Standard
The prior for the initial conditions 
\begin_inset Formula $y_{0,i}=\theta_{82+i},\,i=1,...,33$
\end_inset

 is constructed as a mixture of Gaussians centered at the trajectories of
 the nominal solution
\begin_inset Formula 
\[
Y_{0}\mid\pi_{0}:=\frac{1}{31}\sum_{t=0}^{30}\mathcal{N}\left(\phi^{\text{nom}}\left(t\right),\,\Sigma\right),\quad i=1,...,33,
\]

\end_inset

and the covariance 
\begin_inset Formula $\Sigma$
\end_inset

 being a diagonal matrix with the component-wise covariance estimates
\begin_inset Formula 
\[
\Sigma_{ii}:=\text{Cov}\left(\left(\phi_{i}^{\text{nom}}\left(t\right)\right)_{t=0}^{30}\right),\quad i=1,...,33.
\]

\end_inset


\end_layout

\begin_layout Standard
The prior for the period length 
\begin_inset Formula $\theta_{116}$
\end_inset

 was chosen to be Gaussian with mean 28.9 days and a standard deviation of
 3.4 days (c.f.
 
\begin_inset CommandInset citation
LatexCommand cite
key "fehring2006variability"

\end_inset

), 
\begin_inset Formula 
\[
T\mid\pi_{0}:=\mathcal{N}\left(28.9,3.4^{2}\right).
\]

\end_inset


\end_layout

\begin_layout Subsection
Sampling
\end_layout

\begin_layout Standard
We will now compute the individual posterior samples
\begin_inset Formula 
\[
\bm{x}^{m}:=\left(x_{i}^{m}\right)_{i=1}^{N}\overset{\text{i.i.d}}{\sim}\rho_{X\mid z^{m}}=\rho_{X\mid\pi_{0}}L\left(\theta\mid z^{m}\right)
\]

\end_inset

for each patient 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Standard
Since all sampled parameters 
\begin_inset Formula $\left(x_{i}\right)_{i=1}^{116}$
\end_inset

 are restricted to 
\begin_inset Formula $\mathbb{R}^{+}$
\end_inset

 but the used AM sampler uses normal proposal densities with global support,
 we first rescale the original parameters using 
\begin_inset Formula $\log:\mathbb{R}^{+}\rightarrow\mathbb{R}$
\end_inset

:
\begin_inset Formula 
\[
\tilde{x}_{i}=\log\left(x_{i}\right),\quad i=1,...,116.
\]

\end_inset

The normal proposals in the log-space now correspond to lognormal proposals
 in the original parameter space.
\end_layout

\begin_layout Standard
However undergoing this transformation we also have to adjust the likelihood
 function according to the change of variables formula:
\begin_inset Formula 
\[
\tilde{L}\left(\tilde{x}\mid z\right)=L\left(\exp\left(\tilde{x}\right)\mid z\right)\prod_{i=1}^{116}\tilde{x}_{i}.
\]

\end_inset


\end_layout

\begin_layout Standard
Chosing the initial value for the Markov chain according to our nominal
 values 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat*}{2}
x_{0,i} & :=\log\theta_{i}^{\text{nom }}, &  & \quad i=1,..,82\\
x_{0,82+i} & :=\log y_{0,i}^{\text{nom}}, &  & \quad i=1,..,33\\
x_{0,116} & :=\log28.9,
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
we may hope to start in a region of high density which henceforth is already
 representative for the target density and thus expect a relatively short
 burnin phase.
\end_layout

\begin_layout Standard
In the first runs the initial covariance matrix 
\begin_inset Formula $\Sigma_{0}$
\end_inset

 of the proposal density for the AM sampler was chosen to be small value
 uniform in each direction.
 Upon later runs we reused the covariance structure 
\begin_inset Formula $\Sigma_{N}$
\end_inset

 of present samplings according to
\begin_inset Formula 
\[
\Sigma_{0}:=\frac{2.38^{2}}{d}\Sigma_{N},
\]

\end_inset

to speed up the adaption process.
\end_layout

\begin_layout Standard
We computed 
\begin_inset Formula $50.000.000$
\end_inset

 samples 
\begin_inset Formula $\bm{x}^{m}$
\end_inset

 of the individual posteriors 
\begin_inset Formula $\rho_{X\mid z^{m}}$
\end_inset

 for each patient 
\begin_inset Formula $m$
\end_inset

 at an average speed of around one million samples per hour and core, of
 which we rejected the first 
\begin_inset Formula $10.000.000$
\end_inset

 as a burn-in phase.
\end_layout

\begin_layout Standard
We applied the convergence diagnostics by Gelman and Rubin
\begin_inset CommandInset citation
LatexCommand cite
key "gelman1992inference"

\end_inset

 and concluded convergence of a parameter if the potential scale reduction
 factors 0.975 quantiles were estimated below 1.2.
 This lead to on average 74 converged parameters (varying per patient).
 The Heidelberger and Welch diagnostic 
\begin_inset CommandInset citation
LatexCommand cite
key "heidelberger1983simulation"

\end_inset

 furthermore assessed componentwise stationarity of the Markov Chains for
 on average 109 parameters.
\end_layout

\begin_layout Standard
Keeping in mind that many of the parameters are probably unidentifiable
 (hence random walking in 100 dimensional space) we consider these as good
 results.
\end_layout

\begin_layout Remark
Since the MCMC sampling, consuming most of the computational time of the
 here proposed prior estimation procedure, is easily parallelizable over
 the individual persons, the proposed scheme is well suited for the use
 in e.g.
 clinical studies, where lots of data is available (c.f.
 
\begin_inset CommandInset citation
LatexCommand cite
key "klebanov2016sysmed"

\end_inset

, Section 4).
\end_layout

\begin_layout Subsection
Prior estimation
\end_layout

\begin_layout Standard
After thinning the individual posterior samples 
\begin_inset Formula $\left(\bm{x}^{m}\right)_{m=1}^{53}$
\end_inset

 for computational feasibility, we combined these in order to obtain a grid
 for the prior estimation.
 
\begin_inset Formula 
\[
\bm{x}^{M}:=\bigcup_{m=1}^{M}\bm{x}^{m}.
\]

\end_inset


\end_layout

\begin_layout Standard
The results below were computed with 
\begin_inset Formula $\#\bm{x}^{M}=2596.$
\end_inset


\end_layout

\begin_layout Standard
Note that these samples, which conform to the average density of the individual
 posterior densities, are 
\begin_inset Formula $\pi_{1}:=\psi\left(\pi_{0}\right)$
\end_inset

 distributed.
 Our experiments have shown that this first EM step 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:emstep"

\end_inset

 is already quite close to the NPMLE.
 Since furthermore the MPLE is expected to be a smoothed version of the
 NPMLE, these points serve as a good basis for the importance sampling:
\begin_inset Formula 
\[
\bm{x}^{M}\sim\psi\left(\pi_{0}\right)\approx\pi_{ML}\approx\pi_{\phi_{I}^{\gamma}}.
\]

\end_inset


\end_layout

\begin_layout Standard
We computed the NPMLE and DS-MLE estimates using 100 iterations of the EM
 algorithm, starting with uniform weights.
\end_layout

\begin_layout Standard
In case of the DS-MLE model we smoothed each data point 
\begin_inset Formula $z_{i}$
\end_inset

 by 50 additional samples.
 The kernel 
\begin_inset Formula $K$
\end_inset

 was chosen to be a multivariate Gaussian with diagonal covariance 
\begin_inset Formula $\Sigma^{K}$
\end_inset

, computed by Silverman's rule of thumb
\begin_inset CommandInset citation
LatexCommand cite
key "Silverm1982"

\end_inset

 for kernel density bandwidth estimation, based on the individual data component
s:
\begin_inset Formula 
\[
\Sigma_{i,i}^{K}=0.9M^{-\frac{1}{5}}\text{Cov}\left(z_{i}^{m}\right)_{m=1}^{M}.
\]

\end_inset


\end_layout

\begin_layout Standard
For the optimization of the MPLE we used the Method of Moving Asymptotes
 algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "svanberg2002class"

\end_inset

, a globally-convergent, gradient-based optimizer.
 To guarantee the constraint 
\begin_inset Formula $\sum_{k=1}^{K}w_{k}=1$
\end_inset

 we used the built in Augmented Lagrangian algorithm.
 This basically amends a penalty to the objective, whose impact is scaled
 based on the current misfit of the constraint.
 We furthermore set the upper bounds for each component to 
\begin_inset Formula $1$
\end_inset

, and the lower bounds to 
\begin_inset Formula $10^{-12}$
\end_inset

 to avoid eventual divisions by zero.
\end_layout

\begin_layout Standard
The penalty constant 
\begin_inset Formula $\gamma$
\end_inset

 is chosen by trial and error, until achieving desired smoothing results.
 It would be interesting to find a rule for its choice, e.g.
 by information theoretic reasons or cross validation.
\end_layout

\begin_layout Subsection
Results
\end_layout

\begin_layout Standard
We will now compare the initial prior 
\begin_inset Formula $\pi_{0}$
\end_inset

 with the NPMLE 
\begin_inset Formula $\pi_{ML}$
\end_inset

, the DS-MLE 
\begin_inset Formula $\pi_{DS}$
\end_inset

 and the information penalized MPLE 
\begin_inset Formula $\pi_{\phi_{I}^{\gamma}}$
\end_inset

 for 
\begin_inset Formula $\gamma=100$
\end_inset

.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:paperplot"

\end_inset

 shows the marginal distributions of the transition rate constant from PrA2
 to SeF1, plotted as a kernel density estimate (KDE) with a bandwidth of
 0.1, of the individual estimated priors and an individual subjects (
\begin_inset Formula $m=14)$
\end_inset

 posteriors.
 It furthermore illustrates the prior respectively posterior predictive
 distribution as density plots with the cohort- respectively individual
 data.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename masterplot.pdf
	lyxscale 50
	width 100text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:paperplot"

\end_inset


\begin_inset Formula $\pi_{0},$
\end_inset

 NPMLE, DS-MLE and MPLE (top to bottom) 
\begin_inset Quotes eld
\end_inset

transition rate constant from PrA2 to SeF1
\begin_inset Quotes erd
\end_inset

-marginals of the respective priors (blue) with posteriors (red) for a single
 subject (left), trajectories sampled from the prior together with the cohort
 data (middle) and the subject data with trajectories sampled from the posterior
 (right).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the prior 
\begin_inset Formula $\pi_{0}$
\end_inset

 we combined the MCMC samples 
\begin_inset Formula $\bm{x}^{M}\sim\pi_{1}$
\end_inset

 with the same amount of independent samples from the true density 
\begin_inset Formula $\pi_{0}$
\end_inset

 and then reweighted this sampling with the inverse of a KDE (with the bandwidth
s chosen according to Silverman's rule of thumb
\begin_inset CommandInset citation
LatexCommand cite
key "Silverm1982"

\end_inset

) at each of the grid-points to approximate a true sampling of 
\begin_inset Formula $\pi_{0}$
\end_inset

, which itself did not lead to any interesting plots for the given, relative
 low sample size.
 The first row can therefore only be seen as an approximation to 
\begin_inset Formula $\pi_{0}$
\end_inset

.
 We can still recognize a lot of unfeasible solutions in the first few days
 but also a diverse posterior plot.
\end_layout

\begin_layout Standard
Looking at the NPMLE (second row), we can observe, keeping in mind that
 we are looking at a KDE, that the estimated prior consists of a few, relatively
 high peaks (
\begin_inset Formula $\max_{i}w_{i}=0.023$
\end_inset

).
 This expected behaviour can also be seen in the prior predictive density
 plot (middle), where the individual trajectories of the prior approximating
 the cohort-data are distinctly visible.
 The corresponding posterior exhibits a single peak (
\begin_inset Formula $\max_{i}w_{i}=0.999$
\end_inset

) for the most likely solution found.
\end_layout

\begin_layout Standard
The DS-MLE (third row) provides a slightly more diverse view with smaller
 peaks (
\begin_inset Formula $\max_{i}w_{i}=0.019$
\end_inset

) then the NPMLE and a smoother prior predictive density plot, but still
 exhibiting a rather erratic variation.
 The posterior is dominated by three peaks (
\begin_inset Formula $\max_{i}w_{i}=0.439$
\end_inset

), 
\end_layout

\begin_layout Standard
Finally the MPLE (last row) provides the smoothest of the estimated priors
 (
\begin_inset Formula $\max_{i}w_{i}=0.014$
\end_inset

), while still sharing the informative peak at around 
\begin_inset Formula $x=15$
\end_inset

 with the NPMLE.
 The prior predictive density plot is also more densely covered in the feasible
 region, indicating a higher explanatory power.
 The individual posterior (right) shares the mode of the NPMLE but also
 offers a lot of additional trajectories as explanation (
\begin_inset Formula $\max_{i}w_{i}=0.258$
\end_inset

).
\end_layout

\begin_layout Standard
We henceforth conclude that the MPLE with the mutual information penalty
 indeed leads to satisfactory results, improving on the overconfidence of
 the MPLE, and being at the very least capable of keeping up with other
 contemporary methods such as DS-MLE.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
We have shown that the MPLE with the information penalty 
\begin_inset Formula $\phi_{I}^{\gamma}$
\end_inset

 is an attractive approach to the empirical Bayes method.
 It bases on natural, information-theoretic considerations and admits the
 desirable property of transformation invariance, generalizing the notion
 of the reference priors to the empirical Bayes framework.
 Due to its concavity the objective is computationally feasible and its
 Monte Carlo approximation enables it's application to high-dimensional
 problems eluding the curse of dimensionality.
\end_layout

\begin_layout Standard
We furthermore showed how to apply the developed methods to a real world
 problem by the means of MCMC sampling, affirming its proficiency in a practical
 scenario.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
appendix
\end_layout

\end_inset


\end_layout

\begin_layout Section
References
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
printbibliography[heading=none]
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "refs"
options "plain"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Standard
The introduced algorithms were implemented in Julia, a modern programming
 language for numerical computing, and bundled in the open-source package
 
\emph on
GynC.jl
\emph default
, available at 
\begin_inset CommandInset href
LatexCommand href
name "www.github.com/axsk/GynC.jl"
target "https://github.com/axsk/GynC.jl/"

\end_inset

.
\end_layout

\begin_layout Standard
For installation and usage run the following commands in the Julia REPL:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Pkg.clone(https://github.com/axsk/GynC.jl)
\end_layout

\begin_layout Plain Layout

using GynC
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The package was build in a modular way trying to separate the generic empirical
 Bayes methods and the GynC model specifics in their respective own submodules
 and should hence be easily extensible to other additive-error models.
\end_layout

\begin_layout Standard
It also provides a series of Jupyter-notebooks, containing the code which
 was ran to generate the results from Section 5.
\end_layout

\begin_layout Standard
In the following we will give a short overview over the core components
 and methods of the software and mention some implementation details.
 
\end_layout

\begin_layout Subsubsection*
Empirical Bayes
\end_layout

\begin_layout Standard
The code to this module can be found in the 
\begin_inset Formula $\texttt{src/eb}$
\end_inset

 directory.
 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

LikelihoodModel(xs, ys, zs, datas, measerr, zsampledistr)
\end_layout

\end_inset

This is the base-type of the empirical Bayes module, providing the data
 necessary for the empirical Bayes methods.
 It holds the following fields:
\end_layout

\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{xs}
\end_layout

\end_inset

: vector of 
\begin_inset Formula $\mathcal{X}$
\end_inset

-samples 
\begin_inset Formula $\left(x_{k}\right)$
\end_inset

 constituting the grid of the discretization
\end_layout

\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{ys}
\end_layout

\end_inset

: vector of corresponding pushforward under 
\begin_inset Formula $\Phi$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{zs}
\end_layout

\end_inset

: vector of additional 
\begin_inset Formula $\mathcal{Z}$
\end_inset

-samples for the 
\begin_inset Formula $\mathcal{Z}$
\end_inset

-entropy computation (c.f.
 Section 4.1)
\end_layout

\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{datas}
\end_layout

\end_inset

: vector containing the cohort-data
\end_layout

\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{measerr}
\end_layout

\end_inset

: specification of the measurement error for likelihood computation.
 Whilst basic support is added for generic 
\begin_inset Formula $\texttt{Distributions.Distribution}$
\end_inset

, we also provide the type 
\begin_inset Formula $\texttt{MatrixNormalCentered}$
\end_inset

 used for the computation of the likelihoods in our application.
\end_layout

\begin_layout Itemize
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{zsampledistr}
\end_layout

\end_inset

: specification of the distribution used for generating samples for the
 
\begin_inset Formula $\mathcal{Z}$
\end_inset

-entropy computation
\end_layout

\begin_layout Standard
Based on this type the following convenience functions are available
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

em(m::LikelihoodModel, w0, niter)
\end_layout

\end_inset

Perform 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
texttt{niter}
\end_layout

\end_inset

 iterations of the EM-algorithm on model 
\begin_inset Formula $\texttt{m}$
\end_inset

, starting with initial weights 
\begin_inset Formula $\texttt{w0}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

mple(m::LikelihoodModel, w0, niter, reg, h)
\end_layout

\end_inset

Perform a constraint gradient ascent search to optimize the MPLE with 
\begin_inset Formula $\gamma=\texttt{reg}$
\end_inset

, initial weights 
\begin_inset Formula $\texttt{w0}$
\end_inset

, stepsize 
\begin_inset Formula $\texttt{h}$
\end_inset

 and 
\begin_inset Formula $\texttt{niter}$
\end_inset

 iterations.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

optimmple(m, reg, w0)
\end_layout

\end_inset

Perform a constraint optimization using the Method of Moving Asymptotes
 algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "svanberg2002class"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

hz(m,w), dhz(m,w), logl(m,w), dlogl(m,w)
\end_layout

\end_inset

Compute the loglikelihood, 
\begin_inset Formula $\mathcal{Z}$
\end_inset

-entropy and their derivatives for the given model 
\begin_inset Formula $\texttt{m}$
\end_inset

 at weights 
\begin_inset Formula $\texttt{w}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

smoothedmodel(m, mult)
\end_layout

\end_inset

Perform the smoothing of the data and likelihood functions for the DS-MLE.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

likelihoodmat(xs::Vector,ys::Vector,d::Distribution)
\end_layout

\end_inset

Compute the pairwise likelihoods based for a distribution 
\begin_inset Formula $\texttt{d}$
\end_inset

.
 In the case that 
\begin_inset Formula $\texttt{d::MatrixNormalCentered}$
\end_inset

 an optimized version making use of the binomial decomposition and renormalizati
on of the likelihoods for stability is implemented.
 It also handles incomplete data by excluding NaNs of the computations.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

gyncmodel(n)
\end_layout

\end_inset

Generate a GynC-likelihoodmodel with 
\begin_inset Formula $\texttt{n}$
\end_inset

 presampled x-samples.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

gyncmodel(xs, datas; zmult, sigma)
\end_layout

\end_inset

Generate a GynC-likelihoodmodel for the given samples 
\begin_inset Formula $\texttt{xs}$
\end_inset

 and 
\begin_inset Formula $\texttt{datas}$
\end_inset

 with 
\begin_inset Formula $\texttt{zmult}$
\end_inset

 z samples (for the entropy) per x sample and measurement error standard
 deviation 
\begin_inset Formula $\texttt{sigma}$
\end_inset

 times the components magnitude.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

samplepi0(m), samplepi1(m), subsample(m)
\end_layout

\end_inset

Convenience functions for generating respectively loading (precomputed)
 samples corresponding to the prior and posterior distributions, as well
 as for subsampling.
\end_layout

\begin_layout Subsubsection*
GynC
\end_layout

\begin_layout Standard
The code to this module can be found in the 
\begin_inset Formula $\texttt{src/gync}$
\end_inset

 directory.
 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Config
\end_layout

\end_inset

Base-type managing the configuration for the MCMC sampling (patient data,
 measurement error, initial proposal variance for MCMC, adaptivity of MCMC,
 thinning, initial values and priors).
 For more information refer to 
\begin_inset Formula $\texttt{src/gync/model.jl}$
\end_inset

 .
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

logprior(c, x), logpost(c, x), llh(c,x)
\end_layout

\end_inset

Log prior, posterior and likelihood values for given config 
\begin_inset Formula $\texttt{c}$
\end_inset

 and sample 
\begin_inset Formula $\texttt{x}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

sample(config, iters)
\end_layout

\end_inset

Sample 
\begin_inset Formula $\texttt{iters}$
\end_inset

 samples using the the AMM sampler from the Mamba.jl package
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

batch(configs, iters; dir)
\end_layout

\end_inset

Sample the given vector of 
\begin_inset Formula $\texttt{configs}$
\end_inset

 in parallel on a Slurm cluster and store them in the 
\begin_inset Formula $\texttt{dir}$
\end_inset

 directory whenever any of the iterations in the vector 
\begin_inset Formula $\texttt{iters}$
\end_inset

 is reached.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

alldatas()
\end_layout

\end_inset

Return all present cohort-data 
\begin_inset Formula $\bm{z}^{M}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

gync(y0, p, ts)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Compute a trajectory for given initial values 
\begin_inset Formula $\texttt{y0}$
\end_inset

, parameters 
\begin_inset Formula $\texttt{p}$
\end_inset

 and times 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\texttt{ts}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 using the Sundials CVode solver.
\end_layout

\end_body
\end_document
