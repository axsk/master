#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass amsart
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Nonparametric prior estimation from cohort data and its application to Systems
 Biology
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
This thesis will cover the development and application of an empirical Bayes
 method to cohort data.
\end_layout

\begin_layout Standard
Since the invention of the computer Bayesian methods, beforehand often untractab
le to compute, have gained a great amount of importance in the field of
 reverse problems.
\end_layout

\begin_layout Standard
Now, with the Internet of things coming and the progressing digitalisation
 of scienes, healthcare and *, ever greater amounts of data is beeing gathered,
 coining the term big data.
\end_layout

\begin_layout Standard
This poses new challenges for storage and performance, but also introduces
 problems of erroneous, incomplete and diverse/cohort * data raising the
 demand for adjusted analytical methods.
\end_layout

\begin_layout Standard
By its nature the Bayesian formalism is very well suited to handle uncertainty
 and missing information and the here presented method fits (cohort parallelisat
ion..*)
\end_layout

\begin_layout Standard
Taking a nonparametric, sampling based approch allows the application of
 this method to a wide class of problems.
\end_layout

\begin_layout Standard
The Bayesian's strength but also weakness is the need to incorporate prior
 belief/knowledge about the system in question.
 Whilst the choice of the prior remains a question of debate, we will use
 cohort data, e.g.
 measurements of several persons, to improve our prior knowledge by incorporatin
g the individual posteriors into a new, informative, prior.
\end_layout

\begin_layout Section
Theory
\end_layout

\begin_layout Subsection
The Bayesian formalism
\end_layout

\begin_layout Standard
We start by laying out the basic formal tools in the Bayesian setting.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 denote continuous random variables and 
\begin_inset Formula $\rho\left(x,y\right)$
\end_inset

 their joint probability density.
 The 
\emph on
conditional probability
\emph default
 density of 
\begin_inset Formula $Y$
\end_inset

 given the occurence of the value 
\begin_inset Formula $x$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

 is
\begin_inset Formula 
\[
\rho\left(y|X=x\right):=\frac{\rho\left(x,y\right)}{\rho\left(x\right)},
\]

\end_inset

where 
\begin_inset Formula $\rho\left(x\right)$
\end_inset

 is the 
\emph on
marginal density
\emph default
 of 
\begin_inset Formula $X$
\end_inset

, i.e.
 the joint density 
\begin_inset Formula $\rho\left(x,y\right)$
\end_inset

 marginalized over all possible 
\begin_inset Formula $y$
\end_inset

:
\begin_inset Formula 
\[
\rho\left(x\right):=\int_{y}\rho\left(x,y\right)\mathrm{d}y.
\]

\end_inset


\end_layout

\begin_layout Definition
Succesive insertion of these identities leads to the probability density
 form of 
\emph on
Bayes' theorem
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{equation}
\rho\left(x|Y=y\right):=\frac{\rho\left(x,y\right)}{\rho\left(y\right)}=\frac{\rho\left(y|x\right)\rho\left(x\right)}{\rho\left(y\right)}=\frac{\rho\left(y|x\right)\rho\left(x\right)}{\int_{x}\rho\left(y|x'\right)\rho\left(x'\right)\mathrm{d}x'}.\label{eq:bayes}
\end{equation}

\end_inset

This formula, constituting the heart of Bayesian statistics, tells us how
 to reconstruct the 
\emph on
posterior distribution
\emph default
 
\begin_inset Formula $\rho\left(x|y\right)$
\end_inset

 of the unknown parameter 
\begin_inset Formula $X$
\end_inset

 given data 
\begin_inset Formula $Y$
\end_inset

, using the 
\emph on
likelihood
\emph default
 
\begin_inset Formula $\rho\left(y|x\right)$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

 given 
\begin_inset Formula $Y$
\end_inset

 as well as the 
\emph on
prior
\emph default
 
\begin_inset Formula $\rho\left(x\right)$
\end_inset

 reflecting our prior assumptions on the density of 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Standard
Note that for fixed 
\begin_inset Formula $y$
\end_inset

 the prior 
\begin_inset Formula $\rho\left(x\right)$
\end_inset

 and posterior 
\begin_inset Formula $\rho\left(x|y\right)$
\end_inset

 both are probability densities in 
\begin_inset Formula $x$
\end_inset

, whilst the likelihood 
\begin_inset Formula $\rho\left(y|x\right)$
\end_inset

 would be in 
\begin_inset Formula $y$
\end_inset

 but is not in 
\begin_inset Formula $x$
\end_inset

, which is why it is often called the 
\emph on
likelihood function 
\begin_inset Formula $L\left(x\mid y\right)$
\end_inset


\emph default
 for emphasis.
\end_layout

\begin_layout Standard
Also note that the denominator, the 
\emph on
evidence,
\emph default
 does not depend on 
\begin_inset Formula $x$
\end_inset

 and thus is merely a scaling constant, preserving the probability density
 property of having measure one.
 This will come in handy later for Markov Chain Monte Carlo sampling, since
 we will be able to omit it in crucial calculations.
\end_layout

\begin_layout Subsection
The basic model
\end_layout

\begin_layout Standard
Our inference bases on the combination of a deterministic physical model,
 our description of the reality, with a stochastic measurement error and
 the formalism of Bayes'.
 The phyiscal model is represented as a map 
\begin_inset Formula $\Phi:X\rightarrow Y$
\end_inset

, mapping some parameter 
\begin_inset Formula $x\in X\subseteq\mathbb{R}^{n}$
\end_inset

 to a resulting state 
\begin_inset Formula $y\in Y\subseteq\mathbb{R}^{m}$
\end_inset

.
 We furthermore model the the data generating measurement process 
\begin_inset Formula $Z$
\end_inset

 as an independet Gaussian perturbation with known covariance 
\begin_inset Formula $\Sigma$
\end_inset

 of that state:
\begin_inset Formula 
\[
\rho\left(z\mid X=x\right)=\Phi\left(x\right)+E,\quad E\overset{indep.}{\sim}\mathcal{N}\left(0,\Sigma\right)
\]

\end_inset

or shorthand
\begin_inset Formula 
\[
Z\mid X\sim\mathcal{N}\left(\Phi\left(x\right),\Sigma\right).
\]

\end_inset


\end_layout

\begin_layout Standard
This 
\emph on
likelihood model 
\emph default
gives us the means to compute the probabilty of measuring a single measurement
 
\begin_inset Formula $z$
\end_inset

, given the underlying parameter 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Standard
Assuming the 
\emph on
prior distribution
\emph default
 
\begin_inset Formula $X$
\end_inset

 was known, this would enable us to compute the 
\emph on
posterior 
\begin_inset Formula $X(x|z)$
\end_inset

 
\emph default
given some measurement 
\begin_inset Formula $z$
\end_inset

 by straightforward application of the Bayes' theorem 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:bayes"

\end_inset

.
\end_layout

\begin_layout Standard
Note also that whilst this model is extensible to multiple measurements
 using the product of likelihoods as likelihood, this is only valid under
 the assumption of identically distributed likelihoods, which corresponds
 to the assumption that the same parameter 
\begin_inset Formula $x$
\end_inset

 is beeing responsible for the different measurements.
 This would be the right model if all measurements result from the same
 realization of 
\begin_inset Formula $X$
\end_inset

 (the same subject), but is wrong assuming different measurements correspond
 to an independant draws from the prior 
\begin_inset Formula $X$
\end_inset

 (multiple subjects).
\end_layout

\begin_layout Subsection
The empirical Bayes model
\end_layout

\begin_layout Standard
Since in general the 
\emph on
prior of 
\emph default

\begin_inset Formula $X$
\end_inset

 cannot be assumed to be known a number of different methods have been establish
ed for estimating this prior based on empirical cohort data, giving rise
 to the so called 
\emph on
empirical Bayes methods.
 
\emph default
In that context we extend the model by letting the prior 
\begin_inset Formula $p\left(x\right)$
\end_inset

be a parameter itself, conditioned on a hyperparater 
\begin_inset Formula $\Pi$
\end_inset

, the prior of priors, resulting in the hierarchical model 
\begin_inset Formula $\Pi\rightarrow X\rightarrow Z$
\end_inset

.
\end_layout

\begin_layout Standard
The marginal likelihood of a prior
\begin_inset Formula $\Pi=\pi$
\end_inset

 given a single measurement 
\begin_inset Formula $z$
\end_inset

 is then given by 
\begin_inset Formula 
\[
p_{Z}\left(z\mid\pi\right)=\int_{\mathcal{X}}p_{Z}\left(z\mid x\right)\pi\left(x\right)\mathrm{d}x.
\]

\end_inset


\end_layout

\begin_layout Standard
Since this likelihood, in contrast to the basic Bayesian model above, does
 not depend on a specific realization of 
\begin_inset Formula $X$
\end_inset

 anymore, this allows us this allows us to handle multiple measurements
 coming from different samples of 
\begin_inset Formula $X$
\end_inset

 correctly using the product distribution:
\end_layout

\begin_layout Definition
For finite data/measurements 
\begin_inset Formula $\bm{z}=\left(z_{m}\in\mathcal{Z}\right)_{m=1,...,M}$
\end_inset

 we define the likelihood of a prior 
\begin_inset Formula $\pi$
\end_inset

 as
\begin_inset Formula 
\[
p\left(\bm{z}|\pi\right):=\prod_{m=1}^{M}p_{Z}\left(z_{m}\mid\Pi=\pi\right)
\]

\end_inset


\end_layout

\begin_layout Definition
or alternatively in its renomalized logarithmic form as 
\emph on
finite data log-likelihood 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
name: log-likelihood?
\end_layout

\end_inset


\emph default

\begin_inset Formula 
\[
L_{cd}\left(\pi\mid\bm{z}\right)=\frac{1}{M}\log p\left(\bm{z}|\pi\right)=\frac{1}{M}\sum_{m=1}^{M}\log p_{Z}\left(z_{m}\mid\pi\right).
\]

\end_inset


\end_layout

\begin_layout Definition
For 
\begin_inset Quotes eld
\end_inset

infinite data/measurements
\begin_inset Quotes erd
\end_inset

, represented in the form of a probabilty distribution 
\begin_inset Formula $p_{Z}$
\end_inset

, we define the corresponding 
\emph on
infinite data log-likelihood 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
name: log-likelihood?
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\emph on
\begin_inset Formula 
\[
L_{cc}\left(\pi\mid p_{Z}\right):=\int p_{Z}\left(z\right)\log p_{Z}\left(z\mid\pi\right)\mathrm{d}z.
\]

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $L_{cd}$
\end_inset

 may be seen as a special case of 
\begin_inset Formula $L_{cc}$
\end_inset

 with the 
\emph on
empirical distribution
\emph default
, let us first examine the continious problem and a later limit theorem
 will ...
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
hmmm
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Before we continue with our initial goal of estimating the prior we first
 have to digress to the topic of identifiability.
\end_layout

\begin_layout Subsection
Identifiability
\begin_inset CommandInset label
LatexCommand label
name "sub:Identifiability"

\end_inset


\end_layout

\begin_layout Standard
We call a likelihood model 
\begin_inset Formula $p_{Z}\left(\cdot\mid\pi\right)$
\end_inset

 identifiable, if
\begin_inset Formula 
\begin{equation}
p_{Z}\left(\cdot\mid\pi\right)=p_{Z}\left(\cdot\mid\pi'\right)\Rightarrow\pi=\pi'.\label{eq:ident}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
If this condition is not fulfilled, different priors can lead to the same
 measurements and therefore we cannot expect to reconstruct the correct
 one from the given data.
 The best we can hope for in this case is to recover a candidate of the
 equivalence class of priors leading to the same measurements, specified
 via the equivalence relation:
\begin_inset Formula 
\begin{equation}
\pi\sim\pi':\iff\left\Vert p_{Z}\left(\cdot\mid\Pi=\pi\right)-p_{Z}\left(\cdot\mid\Pi=\pi'\right)\right\Vert _{L^{1}\left(\mathcal{Z}\right)}=0.\label{eq:equiv}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Prior estimation
\end_layout

\begin_layout Standard
The following proposition shows that optimizing the 
\emph on
infinite data likelihood
\emph default
 criterion indeed leads to restoration of the desired true prior up to the
 equivalence considerations from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sub:Identifiability"

\end_inset

.
\end_layout

\begin_layout Proposition
Let 
\begin_inset Formula $\pi\in\mathcal{M}_{1}\left(\mathcal{X}\right)$
\end_inset

 be a globally supported probability density function.
 Then 
\begin_inset Formula $p_{Z}\left(\cdot\mid\Pi=\pi\right)=p_{Z}$
\end_inset

 if and only if 
\begin_inset Formula $\pi$
\end_inset

 maximizes 
\begin_inset Formula $L_{cc}\left(\pi\right)$
\end_inset

.
\end_layout

\begin_layout Proof
The cross entropy of the two densities is minimal if and only if the two
 densities agree modulo 
\begin_inset Formula $\sim$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Ausfuehrlicher
\end_layout

\end_inset


\end_layout

\begin_layout Proof
In other words, maximization of 
\begin_inset Formula $L_{cc}$
\end_inset

 gives us the prior that reconstructs the true evidence 
\begin_inset Formula $p_{Z}$
\end_inset

, which according to the identifiability considerations is all we can hope
 for.
\end_layout

\begin_layout Subsection
Regularization
\end_layout

\begin_layout Standard
NPMLE
\end_layout

\begin_layout Subsection
Numerical realization
\end_layout

\begin_layout Standard
Since we cannot compute the occuring integrals exactly in general, we propose
 the following approximations to discretize the spaces 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\mathcal{Z}$
\end_inset

:
\end_layout

\begin_layout Enumerate
Seeing that in reality only finitely many measurements 
\begin_inset Formula $\bm{z}$
\end_inset

 are available, we approximate the density 
\begin_inset Formula $p_{Z}$
\end_inset

 by
\begin_inset Formula 
\[
p_{Z}\approx\frac{1}{\#\bm{z}}\sum_{z\in\bm{z}}\delta_{z}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
For the integration over the potentially high-dimensional parameter space
 
\begin_inset Formula $\mathcal{X}$
\end_inset

 we choose another Monte-Carlo approximation, corresponding to an importance
 sampling 
\begin_inset Formula $\bm{x=}\left(x_{k}\in\mathcal{X}\right)_{k=1}^{K}$
\end_inset

 with weights 
\begin_inset Formula $\bm{w}=\left(w_{k}\right)_{k=1}^{K},\quad w_{k}\ge0,\quad\sum_{k=1}^{K}w_{k}=1$
\end_inset

:
\begin_inset Formula 
\[
\pi\approx\sum_{k=1}^{K}w_{k}\delta_{x_{k}}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Markov Chain Monte Carlo
\end_layout

\begin_layout Standard
The numerical scheme depends on the choice of the importance samplingg 
\begin_inset Formula $\bm{x}$
\end_inset

.
 Whilst an equidistant grid may work well for small dimensional parameter
 spaces 
\begin_inset Formula $\mathcal{X}$
\end_inset

, the number of samples/gridpoints increases exponentially with the dimension
 of 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 This so called curse of dimensionality suggests the use of Markov Chain
 Monte Carlo (MCMC) sampling, a popular sampling scheme for probability
 densities 
\begin_inset Formula $f$
\end_inset

 defined over high-dimensional spaces.
\end_layout

\begin_layout Standard
The basic idea of MCMC methods revolves around constructing an ergodic Markov
 Chain, a stochastic process whose conditional probability for future states
 depends only on the current state, which has the desired taget-density
 
\begin_inset Formula $f$
\end_inset

 as stationary density.
 In the limit of high sample sizes the samples from the Markov Chain then
 are distributed according to 
\begin_inset Formula $f$
\end_inset

.
\end_layout

\begin_layout Standard
The probably most common MCMC scheme is the Metropolisâ€“Hastings (MH) algorithm.
\end_layout

\begin_layout Standard
..
 proposal/acceptance/...
\end_layout

\begin_layout Standard
Since the quality/speed of the approximation depends on the chosen proposal
 distribution, we will make use of the AMM algorithm...
\end_layout

\begin_layout Section
Application
\end_layout

\begin_layout Subsection
The problem
\end_layout

\begin_layout Standard
In the following we wiwe consider a model for the human menstrual cycle,
 named GynCycle (roeblitz), a system of 33 ordinary differential equations
 with 82 parameters 
\begin_inset Formula $\Theta\in\mathbb{R}_{+}^{82}$
\end_inset

.
\end_layout

\begin_layout Subsection
Sampling
\end_layout

\begin_layout Standard
Since the parameters are restricted to 
\begin_inset Formula $\mathbb{R}^{+}$
\end_inset

 but the used AMM sampler uses Normal proposal densities, we first rescale
 the original parameters using 
\begin_inset Formula $\log:\mathbb{R}^{+}\rightarrow\mathbb{R}$
\end_inset

:
\begin_inset Formula 
\[
\tilde{\Theta_{i}}=\log\left(\Theta_{i}\right),\quad i=1,...,82
\]

\end_inset


\end_layout

\begin_layout Standard
Note that this transformation also transforms the corresponding likelihoods
 according to ...
\end_layout

\begin_layout Subsection
Prior estimation
\end_layout

\begin_layout Subsection
Results
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Section*
Appendix
\end_layout

\begin_layout Subsection*
Implementation
\end_layout

\end_body
\end_document
