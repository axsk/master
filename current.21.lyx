#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass amsart
\begin_preamble
\DeclareMathOperator{\Ima}{Im}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Nonparametric prior estimation from cohort data and its application to Systems
 Biology
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
This thesis will cover the development and application of an empirical Bayes
 method to cohort data.
\end_layout

\begin_layout Standard
Since the invention of the computer Bayesian methods, beforehand often untractab
le to compute, have gained a great amount of importance in the field of
 reverse problems.
\end_layout

\begin_layout Standard
Now, with the Internet of things coming and the progressing digitalisation
 of scienes, healthcare and *, ever greater amounts of data is beeing gathered,
 coining the term big data.
\end_layout

\begin_layout Standard
This poses new challenges for storage and performance, but also introduces
 problems of erroneous, incomplete and diverse/cohort * data raising the
 demand for adjusted analytical methods.
\end_layout

\begin_layout Standard
By its nature the Bayesian formalism is very well suited to handle uncertainty
 and missing information and the here presented method fits (cohort parallelisat
ion..*)
\end_layout

\begin_layout Standard
Taking a nonparametric, sampling based approch allows the application of
 this method to a wide class of problems.
\end_layout

\begin_layout Standard
The Bayesian's strength but also weakness is the need to incorporate prior
 belief/knowledge about the system in question.
 Whilst the choice of the prior remains a question of debate, we will use
 cohort data, e.g.
 measurements of several persons, to improve our prior knowledge by incorporatin
g the individual posteriors into a new, informative, prior.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
todos:
\end_layout

\begin_layout Plain Layout
check p/
\begin_inset Formula $\rho$
\end_inset


\end_layout

\begin_layout Plain Layout
check rand.
 variable vs realization vs density
\end_layout

\end_inset


\end_layout

\begin_layout Section
Theory
\end_layout

\begin_layout Subsection
The Bayesian formalism
\end_layout

\begin_layout Standard
We start by laying out the basic formal tools in the Bayesian setting.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 denote continuous random variables and 
\begin_inset Formula $\rho_{X,Y}\left(x,y\right)$
\end_inset

 their joint probability density.
 The 
\emph on
conditional probability
\emph default
 density of 
\begin_inset Formula $Y$
\end_inset

 given the value 
\begin_inset Formula $x$
\end_inset

 for 
\begin_inset Formula $X$
\end_inset

 is
\begin_inset Formula 
\[
\rho_{Y\mid X}\left(y|x\right):=\frac{\rho_{X,Y}\left(x,y\right)}{\rho_{X}\left(x\right)},
\]

\end_inset

where 
\begin_inset Formula $\rho_{X}\left(x\right)$
\end_inset

 is the 
\emph on
marginal density
\emph default
 of 
\begin_inset Formula $X$
\end_inset

 defined as the joint density 
\begin_inset Formula $\rho\left(x,y\right)$
\end_inset

 marginalized over all possible 
\begin_inset Formula $y$
\end_inset

:
\begin_inset Formula 
\[
\rho_{X}\left(x\right):=\int_{y}\rho\left(x,y\right)\mathrm{d}y.
\]

\end_inset


\end_layout

\begin_layout Definition
Succesive insertion of these identities leads to the probability density
 form of 
\emph on
Bayes' theorem
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{equation}
\rho_{X,Y}\left(x|y\right):=\frac{\rho_{X,Y}\left(x,y\right)}{\rho_{Y}\left(y\right)}=\frac{\rho_{Y\mid X}\left(y|x\right)\rho_{X}\left(x\right)}{\rho_{Y}\left(y\right)}=\frac{\rho_{Y\mid X}\left(y|x\right)\rho_{X}\left(x\right)}{\int_{x}\rho_{Y\mid X}\left(y|x'\right)\rho_{X}\left(x'\right)\mathrm{d}x'}.\label{eq:bayes}
\end{equation}

\end_inset

This formula, constituting the heart of Bayesian statistics, tells us how
 to reconstruct the 
\emph on
posterior distribution
\emph default
 
\begin_inset Formula $\rho_{X\mid Y}\left(x|y\right)$
\end_inset

 of the unknown parameter 
\begin_inset Formula $X$
\end_inset

 given data 
\begin_inset Formula $Y$
\end_inset

, using the 
\emph on
likelihood
\emph default
 
\begin_inset Formula $\rho_{Y\mid X}\left(y|x\right)$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

 given 
\begin_inset Formula $Y$
\end_inset

 as well as the 
\emph on
prior
\emph default
 
\begin_inset Formula $\rho_{X}\left(x\right)$
\end_inset

 reflecting our prior assumptions on the density of 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Standard
Note that for fixed 
\begin_inset Formula $y$
\end_inset

 the prior 
\begin_inset Formula $\rho_{X}\left(x\right)$
\end_inset

 and posterior 
\begin_inset Formula $\rho_{X|Y}\left(x|y\right)$
\end_inset

 both are probability densities in 
\begin_inset Formula $x$
\end_inset

, whilst the likelihood 
\begin_inset Formula $\rho_{Y\mid X}\left(y|x\right)$
\end_inset

 would be in 
\begin_inset Formula $y$
\end_inset

 but is not in 
\begin_inset Formula $x$
\end_inset

, which is why it is often called the 
\emph on
likelihood function 
\begin_inset Formula $L\left(x\mid y\right)$
\end_inset


\emph default
 for emphasis.
\end_layout

\begin_layout Standard
Also note that the denominator, the 
\emph on
evidence,
\emph default
 does not depend on 
\begin_inset Formula $x$
\end_inset

 and thus is merely a scaling constant, preserving the probability density
 property of having measure one.
 This will come in handy later for Markov Chain Monte Carlo sampling, since
 we will be able to omit it in crucial calculations.
\end_layout

\begin_layout Subsection
The likelihood model
\end_layout

\begin_layout Standard
Our inference bases on the combination of a deterministic physical model,
 our description of the reality, with a stochastic measurement error and
 the formalism of Bayes'.
 The phyiscal model is represented as a map 
\begin_inset Formula $\Phi:\mathcal{X}\subseteq\mathbb{R}^{n}\rightarrow\mathcal{Y}\subseteq\mathbb{R}^{m}$
\end_inset

, mapping some parameter 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

 to a resulting state 
\begin_inset Formula $y\in\mathcal{Y}$
\end_inset

.
 We furthermore model the the data generating measurement process 
\begin_inset Formula $Z$
\end_inset

 as an independent Gaussian perturbation with prescribed covariance 
\begin_inset Formula $\Sigma$
\end_inset

 of that state:
\begin_inset Formula 
\[
\rho_{Z\mid X}\left(z\mid x\right)=\Phi\left(x\right)+E,\quad E\overset{indep.}{\sim}\mathcal{N}\left(0,\Sigma\right)
\]

\end_inset

or shorthand
\begin_inset Formula 
\[
Z\mid X\sim\mathcal{N}\left(\Phi\left(X\right),\Sigma\right),
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathcal{\mathcal{N}}\left(y,\,\Sigma\right)$
\end_inset

 denotes the Normal distribution with mean 
\begin_inset Formula $y$
\end_inset

 and covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

.
\end_layout

\begin_layout Standard
This 
\emph on
likelihood model 
\emph default
gives us the means to compute the probabilty of measuring a single measurement
 
\begin_inset Formula $z$
\end_inset

, given the underlying parameter 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Standard
Assuming the 
\emph on
prior distribution
\emph default
 
\begin_inset Formula $X$
\end_inset

 was known, this would enable us to compute the 
\emph on
posterior 
\begin_inset Formula $X(x|z)$
\end_inset

 
\emph default
given some measurement 
\begin_inset Formula $z$
\end_inset

 by straightforward application of the Bayes' theorem 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:bayes"

\end_inset

.
\end_layout

\begin_layout Standard
Note also that whilst this model is extensible to multiple measurements
 using the product of likelihoods as likelihood, this is only valid under
 the assumption of identically distributed likelihoods, which corresponds
 to the assumption that the same parameter 
\begin_inset Formula $x$
\end_inset

 is underlying the different measurements.
 This would be the right model if all measurements result from the same
 realization of 
\begin_inset Formula $X$
\end_inset

 (the same subject), but is wrong assuming different measurements correspond
 to independant draws from the prior 
\begin_inset Formula $X$
\end_inset

 (multiple subjects).
\end_layout

\begin_layout Subsection
The empirical Bayes model
\end_layout

\begin_layout Standard
robbins efron emalgo
\end_layout

\begin_layout Standard
Since in general the 
\emph on
prior 
\emph default

\begin_inset Formula $X$
\end_inset

 cannot be assumed to be known a number of different methods have been establish
ed for estimating this prior based on empirical cohort data, giving rise
 to the so called 
\emph on
empirical Bayes methods.
 
\emph default
In that context we extend the model by conditioning the prior 
\begin_inset Formula $\rho_{X}$
\end_inset

 on a hyperparater 
\begin_inset Formula $\Pi$
\end_inset

, the prior of priors if one likes, resulting in the hierarchical model
 
\begin_inset Formula $\Pi\rightarrow X\rightarrow Z$
\end_inset

, which we will refer to as the 
\emph on
hyperparametric model
\emph default
.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
list characterizing equations here?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Most literature confines itself to (finite dimensional) parametric empirical
 Bayes methods, characterized by considering parametrized families of distributi
ons for the priors, e.g.
 
\begin_inset Formula 
\[
\rho_{X|\pi}=\mathcal{N}\left(\pi,\,I\right),\,\pi\in\text{Im}\left(\Pi\right)=\mathbb{R}^{n},
\]

\end_inset


\end_layout

\begin_layout Standard
since these may admit explicit formulas for prior point estimates if the
 likelihood model and prior families admit simple forms, as well as these
 usually regularization issues.
 We aim at a more general solution to the inference problem by allowing
 arbitrary distributions as priors, i.e.
 
\begin_inset Formula 
\[
\rho_{X\mid\pi}=\pi,\,\pi\in\text{Im}\left(\Pi\right)=\mathcal{M}_{1}\left(\mathcal{X}\right):=\left\{ \rho\in L^{1}\left(\mathcal{X}\right)\mid\rho\ge0,\,\left\Vert \rho\right\Vert _{L^{1}}=1\right\} ,
\]

\end_inset

resulting in a 
\emph on
nonparametric empirical Bayes 
\emph default
method.
\end_layout

\begin_layout Standard
The marginal likelihood of a prior 
\begin_inset Formula $\Pi=\pi$
\end_inset

 given a single measurement 
\begin_inset Formula $z$
\end_inset

 is then given by 
\begin_inset Formula 
\[
L\left(\pi\mid z\right)=\rho_{Z\mid\pi}\left(z\right)=\int_{\mathcal{X}}\rho_{Z\mid x}\left(z\right)\pi\left(x\right)\mathrm{d}x.
\]

\end_inset


\end_layout

\begin_layout Standard
Since this likelihood, in contrast to the basic Bayesian model above, does
 not depend on a specific realization of the latent variable 
\begin_inset Formula $X$
\end_inset

 anymore, this allows us to handle multiple measurements coming from different
 samples of 
\begin_inset Formula $X$
\end_inset

 correctly using the product distribution:
\end_layout

\begin_layout Definition
For finite data/measurements 
\begin_inset Formula $\bm{z}^{M}=\left(z_{m}\in\mathcal{Z}\right)_{m=1,...,M}$
\end_inset

 we define the likelihood of a prior 
\begin_inset Formula $\pi$
\end_inset

 as
\begin_inset Formula 
\[
\rho\left(\bm{z}^{M}|\pi\right):=\prod_{m=1}^{M}\rho_{Z}\left(z_{m}\mid\Pi=\pi\right)
\]

\end_inset


\end_layout

\begin_layout Definition
or alternatively in its renomalized logarithmic form as 
\emph on
finite data log-likelihood 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
name: log-likelihood?
\end_layout

\end_inset


\emph default

\begin_inset Formula 
\[
\mathcal{L}_{cd}\left(\pi\mid\bm{z}^{M}\right):=\frac{1}{M}\log\rho\left(\bm{z}^{M}|\pi\right)=\frac{1}{M}\sum_{m=1}^{M}\log\rho_{Z}\left(z_{m}\mid\pi\right).
\]

\end_inset


\end_layout

\begin_layout Definition
For 
\begin_inset Quotes eld
\end_inset

infinite data/measurements
\begin_inset Quotes erd
\end_inset

, represented in the form of a data-generating probabilty distribution 
\begin_inset Formula $p_{Z}$
\end_inset

, we define the corresponding 
\emph on
infinite data log-likelihood 
\end_layout

\begin_layout Standard

\emph on
\begin_inset Formula 
\[
\mathcal{L}_{cc}\left(\pi\mid\rho_{Z}\right):=\int\rho_{Z}\left(z\right)\log\rho_{Z}\left(z\mid\pi\right)\mathrm{d}z.
\]

\end_inset


\end_layout

\begin_layout Standard
The latter definition follows from the former in the limit for 
\begin_inset Formula $m\rightarrow\infty$
\end_inset

 assuming that 
\begin_inset Formula $p_{Z}$
\end_inset

 is indeed the data generating distribution:
\begin_inset Formula 
\[
z_{m}\overset{i.i.d.}{\sim}\rho_{Z}\Rightarrow\mathcal{L}_{cd}\left(\pi\mid\bm{z}^{M}\right)\overset{M\rightarrow\infty}{\longrightarrow}\mathcal{L}_{cc}\left(\pi\mid\rho_{Z}\right)
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
convergence in which sense?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The following proposition demonstrates how we can recover the data underlying
 
\begin_inset Quotes eld
\end_inset

true prior
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\pi^{*}$
\end_inset

 in the infinite data regime by maximizing the corresponding likelihood
 functional 
\begin_inset Formula $L_{cc}.$
\end_inset


\end_layout

\begin_layout Proposition
Let the hyperparametric model be well specified, i.e.
 
\begin_inset Formula 
\[
\exists\pi^{*}\in\mathcal{M}_{1}\left(\mathcal{X}\right):\,\rho_{Z}=\rho_{Z\mid\pi^{*}}
\]

\end_inset

and identifiable 
\begin_inset CommandInset citation
LatexCommand cite
after "chapter 5"
key "van2000asymptotic"

\end_inset


\begin_inset Formula 
\[
\rho_{Z\mid\pi^{'}}=\rho_{Z\mid\pi^{*}}\Rightarrow\pi'=\pi^{*}\quad\forall\pi'\in\mathcal{M}_{1}.
\]

\end_inset

We then have that
\begin_inset Formula 
\[
\pi^{*}=\underset{\pi'\in\mathcal{M}_{1}\left(\mathcal{X}\right)}{\arg\max}L_{cc}\left(\pi'\right).
\]

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Note Note
status open

\begin_layout Plain Layout
todo
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Both of the assumptions arise nather naturally; if the model is not well
 specified this merely means the measured data 
\begin_inset Formula $\rho_{Z}$
\end_inset

 cannot be explained by any prior, thus resulting in an ill-posed problem.
 If on the other hand the model is not identifiable, which by definition
 corresponds to the injectivity of the marginal likelihood function 
\begin_inset Formula $\rho_{Z}$
\end_inset

, there exists another 
\begin_inset Formula $\pi'\ne\pi^{*}$
\end_inset

 inducing the same measurements so we cannot hope to recover the right prior
 candidate from the data.
\end_layout

\begin_layout Standard
This can be retracted though by lifting the inference problem to equivalence
 classes of priors leading to the same measurements,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\pi\sim\pi':\iff\left\Vert \rho_{Z\mid\pi}-\rho_{Z\mid\pi'}\right\Vert _{L^{1}\left(\mathcal{Z}\right)}=0,\label{eq:equiv}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
which is all we can hope for.
\end_layout

\begin_layout Standard
In practice though usually only finite data is available, and even though
 the limiting property 
\begin_inset Note Note
status open

\begin_layout Plain Layout
ref
\end_layout

\end_inset

 might give hope that maximization of 
\begin_inset Formula $L_{cd}$
\end_inset

 might approximate 
\begin_inset Formula $\pi^{*}$
\end_inset

 properly, one can prove 
\begin_inset CommandInset citation
LatexCommand cite
after "Theorem 21"
key "lindsay1995mixture"

\end_inset

 that the maximizer of 
\begin_inset Formula $L_{cd}$
\end_inset

 is a discrete distribution with at most 
\begin_inset Formula $M$
\end_inset

 nodes.
\end_layout

\begin_layout Standard
In the field of machine learning this phenomenom, commonly occuring for
 insufficient data, is referred to as 
\emph on
overfitting
\emph default
 and usually approached by regularization techniques, methods to enforce
 more regular and smooth solutions.
\end_layout

\begin_layout Subsection
Regularization
\end_layout

\begin_layout Standard
To adress this problem we will introduce two such regularization methods,
 the 
\emph on
maximum penalized likelihood estimation
\emph default
 (MPLE), introducing a penaliztion term to the former optimization problem,
 and the 
\emph on
doubly smoothed maximum likelihood estimation 
\emph default
(DSMLE), lifting the problem from the finite to the infinite data regime
 by smoothing the measurements and thus approximating the continious data-genera
ting distribution 
\begin_inset Formula $\rho_{Z}.$
\end_inset


\end_layout

\begin_layout Standard
For a given likelihood function 
\begin_inset Formula $L$
\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
specify
\end_layout

\end_inset

 and a 
\emph on
roughness penalty 
\emph default
(or 
\emph on
regularization term
\emph default
) 
\begin_inset Formula $\Phi:\mathcal{M}_{1}\rightarrow\mathbb{R}$
\end_inset

, responsible for penalizing unsmooth or unwanted solutions with high values,
 the MPLE estimate admits the form 
\begin_inset Formula 
\begin{equation}
\pi_{MPLE}=\underset{\pi}{\arg\max}\log L\left(\pi\right)-\Phi\left(\pi\right).\label{eq:mple}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This approach also allows for an interpretation in the context of the Bayesian
 hyperparametric model by identifying the penalty 
\begin_inset Formula $\Phi$
\end_inset

 with the hyperprior 
\begin_inset Formula $\Pi$
\end_inset

 via 
\begin_inset Formula $\rho_{\Pi}\propto e^{-\Phi}$
\end_inset

.
 The posterior then is
\begin_inset Formula 
\[
\rho_{\Pi\mid Z}\propto L\left(\pi\right)e^{-\Phi\left(\pi\right)}
\]

\end_inset

and thus the 
\emph on
maximum a posteriori 
\emph default
estimate 
\begin_inset Formula $\pi_{MAP}$
\end_inset

 for the hyperparametric model corresponds to the MPLE estimate:
\begin_inset Formula 
\[
\pi_{MAP}=\underset{\pi}{\arg\max}L\left(\pi\right)e^{-\Phi\left(\pi\right)}=\underset{\pi}{\arg\max}\log L\left(\pi\right)-\Phi\left(\pi\right)=\pi_{MPLE}.
\]

\end_inset


\end_layout

\begin_layout Standard
One now might might argue that we started with the question of finding the
 correct prior and just complicated the situation by transferring this problem
 to the question of the correct hyperprior.
 While this may be true we argue that the latter can be tackled from a rather
 abstract, problem independent standpoint, hence leading to a more general
 answer.
\end_layout

\begin_layout Subsection
Choice of the penalty
\end_layout

\begin_layout Standard
Many of the common penalty functions currently in use, penalizing either
 large amplitudes (e.g.
 ridge regression 
\begin_inset CommandInset citation
LatexCommand cite
after "section 1.6"
key "mclachlan2007algorithm"

\end_inset

) or derivatives (c.f.
 
\begin_inset CommandInset citation
LatexCommand cite
key "good1971nonparametric"

\end_inset

) of the prior, are not invariant under reparemerizations of the parameter
 space 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and are rather ad-hoc without a natural derivation.
 Following a more information-theoretic view Good 
\begin_inset CommandInset citation
LatexCommand cite
key "good1963maximum"

\end_inset

 suggested the use of the differential entropy for the penalty
\begin_inset Formula 
\[
\Phi\left(\pi\right):=-\gamma H_{X}\left(\pi\right):=\gamma\int_{\mathcal{X}}\rho_{X\mid\pi}\left(x\right)\log\rho_{X\mid\pi}\left(x\right)\mathrm{d}x,
\]

\end_inset

 with 
\begin_inset Formula $\gamma\in\mathbb{R}^{+}$
\end_inset

 beeing a parameter determining the degree of smoothing due to this penalty.
 This prior however is still variant under reparametrizations of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 due to the log-nonlinearity.
 This means that if two scientists estimate the prior using equivalent models,
 e.g.
 by using different systems of units, they would end up with different estimates.
 Hence this penalty does not rectify the problem of subjectivity in the
 Bayesian method.
 We therefore look for a penalty which is invariant under coordinate transformat
ions.
\end_layout

\begin_layout Standard
Embracing the information theoretic approach we therefore propose the use
 of the 
\emph on
mutual information 
\emph default
instead of the entropy for the penalty:
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $X:\,\Omega\rightarrow\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $Y:\,\Omega\rightarrow\mathcal{Y}$
\end_inset

 be two continuous random variables, 
\begin_inset Formula $\rho_{X,Y}$
\end_inset

 their joint probability density and 
\begin_inset Formula $\rho_{X},\,\rho_{Y}$
\end_inset

 their respective marginal densities.
 Their mutual information is defined as
\begin_inset Formula 
\begin{align*}
\mathcal{I}\left(X;Y\right): & =\int_{\mathcal{Y}}\int_{\mathcal{X}}\rho_{X,Y}\left(x,y\right)\log\left(\frac{\rho_{X,Y}\left(x,y\right)}{\rho_{X}\left(x\right)\rho_{Y}\left(y\right)}\right)\mathrm{d}x\mathrm{dy}\\
 & =\mathbb{E}_{y\sim Y}\left[D_{KL}\left(\rho_{X\mid Y}\left(\cdot\mid y\right)\parallel\rho_{X}\left(\cdot\right)\right)\right]\\
 & =H\left(Y\right)-H\left(Y\mid X\right)
\end{align*}

\end_inset

with 
\begin_inset Formula $D_{KL}$
\end_inset

 denoting the Kullback-Leibler divergence 
\begin_inset Formula 
\[
D_{KL}\left(\rho_{A}\parallel\rho_{B}\right):=\int_{E}\rho_{A}\left(x\right)\log\frac{\rho_{A}\left(x\right)}{\rho_{B}\left(x\right)}\mathrm{d}x,\quad\rho_{A},\rho_{B}\,\text{probability densities on }E
\]

\end_inset

 and 
\begin_inset Formula 
\begin{eqnarray*}
H\left(Y\right) & := & -\int_{Z}\\
H\left(Y\mid X\right) & := & -\int\int
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:invariance"

\end_inset

Let 
\begin_inset Formula $X$
\end_inset

, 
\begin_inset Formula $Y$
\end_inset

 as above, and 
\begin_inset Formula $\varphi^{-1}:\,\mathcal{X}\rightarrow\tilde{\mathcal{X}}$
\end_inset

, 
\begin_inset Formula $\psi^{-1}:\,\mathcal{Y}\rightarrow\tilde{\mathcal{Y}}$
\end_inset

 be diffeomorphisms defining the coordinate transformations and corresponding
 transformed random variables 
\begin_inset Formula $\tilde{X},\,\tilde{Y}$
\end_inset

 with their densities
\begin_inset Formula 
\[
\rho_{\tilde{X},\tilde{Y}}\left(\tilde{x},\tilde{y}\right):=\rho_{X,Y}\left(\varphi\left(\tilde{x}\right),\psi\left(\tilde{y}\right)\right)\left|D\varphi\left(\tilde{x}\right)\right|\left|D\psi\left(\tilde{y}\right)\right|
\]

\end_inset


\begin_inset Formula 
\[
\rho_{\tilde{X}}\left(\tilde{x}\right):=\rho_{X}\left(\varphi\left(\tilde{x}\right)\right)\left|D\varphi\left(\tilde{x}\right)\right|,\,\quad\rho_{\tilde{Y}}\left(\tilde{y}\right):=\rho_{Y}\left(\psi\left(\tilde{y}\right)\right)\left|D\psi\left(\tilde{x}\right)\right|
\]

\end_inset


\end_layout

\begin_layout Lemma
The mutual information 
\begin_inset Formula $\mathcal{I}\left(X;Y\right)$
\end_inset

 is invariant under these coordinate transformations of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\mathcal{Y}.$
\end_inset


\end_layout

\begin_layout Proof
According to the change of variables formula 
\begin_inset Formula 
\begin{align*}
 & \ \mathcal{I}\left(X;Y\right)\\
= & \int_{\mathcal{Y}}\int_{X}\rho_{X,Y}\left(x,y\right)\log\left(\frac{\rho_{X,Y}\left(x,y\right)}{\rho_{X}\left(x\right)\rho_{Y}\left(y\right)}\right)\mathrm{d}x\mathrm{dy}.\\
= & \int_{\mathcal{\tilde{Y}}}\int_{\tilde{\mathcal{X}}}\rho_{X,Y}\left(\varphi\left(\tilde{x}\right),\psi\left(\tilde{y}\right)\right)\log\left(\frac{\rho_{X,Y}\left(\varphi\left(\tilde{x}\right),\psi\left(\tilde{y}\right)\right)}{\rho_{X}\left(\varphi\left(\tilde{x}\right)\right)\rho_{Y}\left(\psi\left(\tilde{y}\right)\right)}\right)\left|D\varphi\left(\tilde{x}\right)\right|\left|D\psi\left(\tilde{y}\right)\right|\mathrm{d}\tilde{x}\mathrm{d\tilde{y}}\\
= & \int_{\mathcal{\tilde{Y}}}\int_{\tilde{\mathcal{X}}}\rho_{\tilde{X},\tilde{Y}}\left(\tilde{x},\tilde{y}\right)\log\left(\frac{\rho_{\tilde{X},\tilde{Y}}\left(\tilde{x},\tilde{y}\right)}{\rho_{\tilde{X}}\left(\tilde{x}\right)\rho_{\tilde{Y}}\left(\tilde{y}\right)}\right)\mathrm{d}\tilde{x}\mathrm{d\tilde{y}}\\
= & \ \mathcal{I}(\tilde{X};\tilde{Y})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The mutual information quantifies the 
\begin_inset Quotes eld
\end_inset

amount of information
\begin_inset Quotes erd
\end_inset

 that one random variable shares with the respective other, expressed by
 the weighted information content of the their joint distribution (
\begin_inset Formula $\rho_{X,Y}$
\end_inset

) relative to their joint distribution if they were independent (
\begin_inset Formula $\rho_{X}\rho_{Y})$
\end_inset

.
\end_layout

\begin_layout Standard
We can gain further insights into its meaning by expressing it in terms
 of another fundamental information-theoretic quantity, the Kullback-Leibler
 divergence.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $A,\,B:\,\Omega\rightarrow E$
\end_inset

 be two continuous random variables with respective density functions 
\begin_inset Formula $\rho_{A}$
\end_inset

, 
\begin_inset Formula $\rho_{B}$
\end_inset

.
 The
\emph on
 Kullback-Leibler divergence 
\emph default
from 
\begin_inset Formula $B$
\end_inset

 to 
\begin_inset Formula $A$
\end_inset

 is defined to be
\end_layout

\begin_layout Definition
\begin_inset Formula 
\[
D_{KL}\left(A\parallel B\right):=\int_{E}\rho_{A}\left(x\right)\log\frac{\rho_{A}\left(x\right)}{\rho_{B}\left(x\right)}\mathrm{d}x.
\]

\end_inset


\end_layout

\begin_layout Standard
The Kullbach-Leibler divergence (also called 
\emph on
information gain 
\emph default
or 
\emph on
relative entropy
\emph default
) is a measure for the loss of information when considering 
\begin_inset Formula $B$
\end_inset

 as a approximation to 
\begin_inset Formula $A$
\end_inset

, or consequently in the Bayesian context it is the gain of information
 revising one's beliefs from the prior 
\begin_inset Formula $B$
\end_inset

 to the posterior 
\begin_inset Formula $A$
\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
quelle
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Then, since 
\begin_inset Formula $\rho_{X,Y}\left(x,y\right)/\rho_{Y}\left(y\right)=\rho_{X\mid Y}\left(x\mid y\right),$
\end_inset

 we have 
\begin_inset Formula 
\begin{align*}
\mathcal{I}\left(X;Y\right) & =\int_{\mathcal{Y}}\rho_{Y}\left(y\right)\int_{\mathcal{X}}\rho_{X\mid Y}\left(x\mid y\right)\log\left(\frac{\rho_{X\mid Y}\left(x\mid y\right)}{\rho_{X}\left(x\right)}\right)\mathrm{d}x\mathrm{dy}\\
 & =\mathbb{E}_{y\sim Y}\left[D_{KL}\left(\rho_{X\mid Y}\left(\cdot\mid y\right)\parallel\rho_{X}\left(\cdot\right)\right)\right],
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
that is the mutual information of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 corresponds to the expected information gain from the prior to the posterior
 over 
\begin_inset Formula $\mathcal{X}$
\end_inset

 when the measurements are 
\begin_inset Formula $Y$
\end_inset

-distributed.
\end_layout

\begin_layout Definition
For 
\begin_inset Formula $\gamma>0$
\end_inset

 constant we define the 
\emph on
information penalty
\emph default
 
\begin_inset Formula 
\begin{eqnarray}
\Phi_{I}\left(\pi\right): & = & -\gamma\mathcal{I}\left(X\mid\pi;Z\mid\pi\right)\nonumber \\
 & = & -\gamma\int_{\mathcal{Z}}\int_{\mathcal{X}}\rho_{X\mid\pi}\left(x\right)\rho_{Z\mid x}\left(z\right)\log\left(\frac{\rho_{Z\mid x}\left(z\right)}{\rho_{Z\mid\pi}\left(z\right)}\right)\mathrm{d}x\mathrm{dz}.\label{eq:ipen}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Minimizing this penalty then corresponds to maximizing the amount of shared
 information between the prior-predictive distribution 
\begin_inset Formula $\rho_{Z\mid\pi}$
\end_inset

 and the prior 
\begin_inset Formula $\rho_{X\mid\pi}$
\end_inset

 itself.
 In terms of the Kullback-Leibler formulation this means priors 
\begin_inset Formula $\pi$
\end_inset

 are rewarded by the amount of information gain expected from their induced
 measurements hence leading to non-informativity.
\end_layout

\begin_layout Corollary
The maximum penalized likelihood estimator 
\begin_inset Formula $\pi_{MPLE}$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mple"

\end_inset

 with the information penalty 
\begin_inset Formula $\Phi_{I}$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ipen"

\end_inset

 is invariant under transformations of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\mathcal{Y}$
\end_inset

 as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:invariance"

\end_inset

.
\end_layout

\begin_layout Standard
The 
\end_layout

\begin_layout Standard
In the case of an additive measurement error we can simplify the 
\end_layout

\begin_layout Subsection
Numerical realization
\end_layout

\begin_layout Standard
Since we cannot compute the occuring integrals exactly in general, we propose
 the following approximations to discretize the spaces 
\begin_inset Formula $\mathcal{X}$
\end_inset

 and 
\begin_inset Formula $\mathcal{Z}$
\end_inset

:
\end_layout

\begin_layout Enumerate
Seeing that in reality only finitely many measurements 
\begin_inset Formula $\bm{z}$
\end_inset

 are available, we approximate the density 
\begin_inset Formula $p_{Z}$
\end_inset

 by
\begin_inset Formula 
\[
p_{Z}\approx\frac{1}{\#\bm{z}}\sum_{z\in\bm{z}}\delta_{z}.
\]

\end_inset


\end_layout

\begin_layout Enumerate
For the integration over the potentially high-dimensional parameter space
 
\begin_inset Formula $\mathcal{X}$
\end_inset

 we choose another Monte-Carlo approximation, corresponding to an importance
 sampling 
\begin_inset Formula $\bm{x=}\left(x_{k}\in\mathcal{X}\right)_{k=1}^{K}$
\end_inset

 with weights 
\begin_inset Formula $\bm{w}=\left(w_{k}\right)_{k=1}^{K},\quad w_{k}\ge0,\quad\sum_{k=1}^{K}w_{k}=1$
\end_inset

:
\begin_inset Formula 
\[
\pi\approx\sum_{k=1}^{K}w_{k}\delta_{x_{k}}.
\]

\end_inset


\end_layout

\begin_layout Standard
resulting monte carlo approximations of the likelihoods, entropies and derivativ
es
\end_layout

\begin_layout Standard
gradient ascent in the simplex
\end_layout

\begin_layout Subsection
Markov Chain Monte Carlo
\end_layout

\begin_layout Standard
The quality of the numerical approximation greatly depends on the choice
 of the importance samples 
\begin_inset Formula $\bm{x}$
\end_inset

.
 Whilst an equidistant grid may work well for small dimensional parameter
 spaces 
\begin_inset Formula $\mathcal{X}$
\end_inset

, the number of samples/gridpoints increases exponentially with the dimension
 of 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 This so called curse of dimensionality suggests the use of Markov Chain
 Monte Carlo (MCMC) sampling, a popular sampling scheme for probability
 densities 
\begin_inset Formula $f$
\end_inset

 defined over high-dimensional spaces.
\end_layout

\begin_layout Standard
The basic idea of MCMC methods revolves around constructing an ergodic Markov
 Chain, a stochastic process whose conditional probability for future states
 depends only on the current state, which has the desired taget-density
 
\begin_inset Formula $f$
\end_inset

 as stationary density.
 In the limit of high sample sizes the samples from the Markov Chain then
 are distributed according to 
\begin_inset Formula $f$
\end_inset

.
\end_layout

\begin_layout Standard
The probably most common MCMC scheme is the Metropolis–Hastings (MH) algorithm.
\end_layout

\begin_layout Standard
..
 proposal/acceptance/...
\end_layout

\begin_layout Standard
Since the quality/speed of the approximation depends on the chosen proposal
 distribution, we will make use of the AMM algorithm...
\end_layout

\begin_layout Standard
convergence tests
\end_layout

\begin_layout Section
Application
\end_layout

\begin_layout Subsection
The problem
\end_layout

\begin_layout Standard
In the following we wiwe consider a model for the human menstrual cycle,
 named GynCycle (roeblitz), a system of 33 ordinary differential equations
 with 82 parameters 
\begin_inset Formula $\Theta\in\mathbb{R}_{+}^{82}$
\end_inset

.
\end_layout

\begin_layout Standard
Mention Data
\end_layout

\begin_layout Standard
likelihood model, modeling of period, special care considering nans
\end_layout

\begin_layout Subsection
Sampling
\end_layout

\begin_layout Standard
Since the parameters are restricted to 
\begin_inset Formula $\mathbb{R}^{+}$
\end_inset

 but the used AMM sampler uses Normal proposal densities, we first rescale
 the original parameters using 
\begin_inset Formula $\log:\mathbb{R}^{+}\rightarrow\mathbb{R}$
\end_inset

:
\begin_inset Formula 
\[
\tilde{\Theta_{i}}=\log\left(\Theta_{i}\right),\quad i=1,...,82
\]

\end_inset


\end_layout

\begin_layout Standard
Note that this transformation also transforms the corresponding likelihoods
 according to ...
\end_layout

\begin_layout Standard
burnin, convergence tests
\end_layout

\begin_layout Subsection
Prior estimation
\end_layout

\begin_layout Subsection
Results
\end_layout

\begin_layout Standard
plot mcmc chain
\end_layout

\begin_layout Standard
paperplot
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "refs"
options "plain"

\end_inset


\end_layout

\begin_layout Section*
Appendix
\end_layout

\begin_layout Subsection*
Implementation
\end_layout

\end_body
\end_document
